# 人工智能导论 复习笔记

> 张泽宇 202201217

- [x] 树搜索和图搜索
- [x] 一致代价搜索
- [x] A* 算法
- [x] 最小最大算法和Alpha-Beta搜索
- [x] 线性回归和线性分类
- [x] ID3和C4.5算法
- [x] 马尔科夫决策过程MDP
- [x] Q-learning(DQN)
- [x] 策略梯度和强化学习
- [x] 多层感知机(MLP)
- [x] 卷积神经网络(CNN)
- [ ] 贝叶斯网络和D-分离条件
- [ ] 生成模型
- [ ] 混合模型
- [ ] 概率主题模型
- [ ] 变分自编码器

[TOC]

## 1. 绪论

### 1.1 人工智能 Artificial Intelligence

+ AI的历史：符号主义，连接主义，统计学派
+ 最早的AI程序：
  + 1952年，跳棋游戏的**前向搜索**（lookahead search）
  + 1956年，**搜索树和启发式搜索**（search tree and heuristics search）和假设-推论树结构
+ AI的诞生：
  + Every aspect of learning or any other feature of intelligence can in principle  be so precisely described that a machine can be made to simulate it（学习的任何方面或智能的任何特征理论上都能被精细表达以至于机器能够模拟该智能特征）
+ 第一个AI冬天：AI需要对偶学习的学习范式
  + 计算算力的限制（Limited computation）和信息数据的限制（Limited information）
+ **基于规则/基于知识的系统**（Knowledge-based Systems）
  + 第一个工业层面的人工智能应用
  + 规则很难表达现实世界的不确定性，容易出现推理过程中的组合爆炸
  + 引发了第二个AI冬天
+ **人工神经网络**
  + 1943，基于感知机学习算法的神经网络
  + 1969，线性模型无法解决异或问题
  + 1980，使用**感受野**（Receptive field）的卷积神经网络
  + 1986，反向传播算法（backpropagation）实现多层神经网络的求解问题

+ 三种智能模型
  + 推理智能（Symbolic AI）：自顶向下地解决问题
  + 底层智能（Neural AI）：自底向上地解决问题
  + 效用函数/价值表达（Statistical AI（：数学严谨性和理论上的保证
  + ![image-20250611194957661](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250611194957661.png)


### 1.2 对AI的两种视角 Two Views of Artificial Intelligence

+ 两种对AI的看法：AI智能体和AI工具

+ AI智能体（Intelligence Agent）：智能体是自主的实体；它使用传感器感知环境（真实的或者模拟的），使用告知星期执行动作并影响环境；智能体学习必须与环境进行交互
  + 思考
    + 知识储存（Keep Knowledge）
    + 推理（Reasoning）
    + 学习（Learning）
  + 行为
    + 感受：计算机视觉（Computer Vision)
    + 动作：机器人（Robotics）
    + 交流：自然语言处理（Natural language processing）
  + 通用人工智能（Artificial General Intelligence, AGI）的进程虽然远，但是快
+ AI工具：利用计算机的专项学习能力，训练适用于特定应用的AI

### 1.3 AI的进步 Advances in AI（略）

### 1.4 AI的挑战 Challenges in AI

+ 安全攸关系统（Critical Systems）：交通、金融、安全、医药、军事。法律
+ 可信人工智能（Trustworthy AI）：旨在像人类一样解决问题的程序和系统，为人们带来好处和便利，而不会造成威胁或伤害的风险
+ 重要因素：可解释性（Explainability）；安全性（Safety）；公平（Fairness）

## 2. 搜索

+ 搜索问题
  + 反射式智能体（Reflex Agents）：输入-函数-单一的动作
  + 规划式智能体（Planning Agents）：输入-函数-动作序列
  + 世界模型：现实世界对动作进行相应的描述；世界模型可以知道智能体进行决策
+ 搜索问题的组成

| 符号            | 名称             | 意义                                             |
| --------------- | ---------------- | ------------------------------------------------ |
| $$S$$           | state space      | 模型的所有状态组成的状态空间                     |
| $$s_0$$         | initial state    | 初态                                             |
| $$A(s)$$        | actions          | 状态上的动作集                                   |
| $$Result(s,a)$$ | transition model | 转移模型或动力学模型，状态在动作上转移的后继函数 |
| $$G(s)$$        | goal test        | 目标测试，测试状态是否是终态                     |
| $$c(s,a,s^,)$$  | action cost      | 动作的代价                                       |

+ 解（solution）是是达到目标状态的动作序列
+ 最优解（optimal solution）是动作代价最少的解
+ 计算状态空间
  + 考虑吃豆人模型，位置120，食物30，敌人位置12，动作NSEW
    + 世界模型：$$120\times 2^{30}\times 12^{2}\times 4$$
    + 寻路算法状态：120
    + 吃豆状态：$$120\times 2^{30}$$  

### 2.1 常规搜索 General Search

+ 状态空间图：搜索问题的模型
  + 节点：状态
  + 有向边：动作
  + 特殊的节点集合：目标状态
+ 每个状态都在搜索过程中只出现**一次**
+ 大部分情况下无法建立完整的状态空间图（不可全知）

#### 2.1.1 树搜索 Tree Search

+ 搜索树：由代表if线的路径构成的树；大部分情况下无法建立完整的搜索树
  + 根节点是初始状态
  + 子节点是对应的状态
  + 从根节点到某个子节点具有唯一的路径
+ 搜索树是在图数据结构的基础上建设的算法表现形式

![image-20250612121503759](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612121503759.png)

+ 一般的树搜索会导致树上出现大量重复的状态和冗余的路径，从而大大增加算法的复杂度

#### 2.1.2 图搜索 Graph Search

![image-20250612121647119](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612121647119.png)

+ 图搜索只将**未被探索过的节点放入边缘集**，从而保证每个状态只在边缘集中出现一次，大大减少状态的数量

### 2.2 无信息搜索 Uninformed Search

#### 2.2.1 深搜和广搜 DFS/BFS

##### DFS

+ 策略：优先探索最深的节点
+ 实现：边缘集是**后进先出栈**
  + 时间复杂度：对于深度为m的搜索树，考虑解出现在第m层$$b^m$$个节点中的随机位置，时间复杂度是$$O(b^m)$$
  + 空间复杂度：对于分支因子为b的搜索树，每一层存储b个同父兄弟节点，空间复杂度为$$O(bm)$$
  + 完备性：有限无环图完备，否则不完备
  + 最优性：和状态出现的顺序相关，寻找最左解而非最优解

##### BFS

+ 策略：优先探索最浅（同层）的节点
+ 实现：边缘集是**先进先出队列**
  + 时间复杂度：取决于最浅解的深度d，要探索比这个节点更浅的所有节点，所以时间复杂度是$$O(b^d)$$
  + 空间复杂度：需要存储同层的所有节点信息，空间复杂度是$$O(b^d)$$
  + 完备性：完备
  + 最优性：如果每条边代价均等，最优；否则用一致代价搜索

##### IDS

+ 策略：分别对每一个深度m执行DFS，到达m若不存在解则强制回退
+ 完备性和最优性：同BFS
+ 时间复杂度：$$\sum_{m=0}^{d}O(b^m)=O(b^d)$$
+ 空间复杂度：$$O(m_{bm\le d})=O(bd)$$

#### 2.2.2 一致代价搜索 UCS

##### UCS

+ 策略：优先探索离已探索集最近的节点（从最短路径开始探索）
+ 实现：边缘集是基于（到当前边缘集节点）**最短路径的优先级队列**，边缘集构成带根节点的搜索等值面
  + 时间复杂度：假设最优解路径代价是$$C^{*}$$，每一层路径代价是$$\varepsilon$$，解出现的期望层数是$$\frac{C^*}{\varepsilon}$$，时间复杂度是$$O(b\frac{C^*}{\varepsilon})$$
  + 空间复杂度：需要搜索同搜索等值面的所有节点信息，空间复杂度是$$O(b\frac{C^*}{\varepsilon})$$
  + 完备性：完备
  + 最优性：可以找到最优解

![image-20250612130441475](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612130441475.png)

![image-20250612130456598](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612130456598.png)

+ 搜索算法的核心区别在于边缘集策略
+ 可以编写搜索算法，使用可变的边缘集对象

### 2.3 启发式搜索 Uninformed Search

+ **启发式**
  + 启发式函数是一个估计一个状态离目标有多近的函数
  + 对于不同的搜索问题，需要单独构造启发式函数
  + 简单的启发式函数包含：曼哈顿距离、欧氏距离

##### 贪心启发式搜索

+ 策略：优先探索离目标最近的节点
+ 实现：边缘集是一个**贪心优先级队列**
  + 完备性：不完备，会陷入局部极值（比如两个节点离目标很近，但不和目标相连）
  + 最优性：不保证最优，会优先采用步长较大的搜索

#### 2.3.1 A*树搜索 A∗ Tree Search

##### A*搜索

+ 策略：维护函数$$f(n)=g(n)+h(n)$$，即`已知路径长度+到目标的距离`
+ 实现：边缘集是**f(n)的小优先级队列**
+ 结束条件：终态（目标状态）**离开**队列

##### A*搜索最优性证明

假设：启发式函数是**可采纳的**意为其不会高估状态到目标的真实代价
$$
0\le h(n) \le h^*(n),[when] \ h^*(n)=[true\  cost]
$$

+ 对于寻路算法，一般认为欧式距离不会高估代价；如果移动方向是NSEW，曼哈顿距离不会高估代价

命题：最优解A一定比次优解B先出队

证明：1. 最优解A的祖先n比最优解A先出队

2. 在祖先出队之后，最优解A一定比次优解B先出队

##### A*搜索高效性证明

+ A*搜索探索所有满足$$g(s)\le g(s_{goal})-h(s)$$的节点，$$h(s)$$越大（约逼近真实成本），需要检查的节点越少，算法复杂度越低；

  + UCS算法的$$h(s)=0$$，所以A*搜索的复杂度一定低于UCS

+ 可接受的启发式通常是**松弛问题**（可采取的动作集更大的问题）的解

  + 例如，对于华容道问题

  | 序号 | 松弛                         | 启发式函数                       |
  | ---- | ---------------------------- | -------------------------------- |
  | 1    | 可以把数字块拿出来放         | 放错的数字块数量                 |
  | 2    | 数字块可以自由移动到隔壁位置 | 数字块到正确位置的曼哈顿距离之和 |

+ 当松弛产生了多个启发式函数时：

  + 如果启发式函数**严格好**：$$\forall n,h_1(n)\ge h_2(n)$$，取严格好的函数
  +  如果不严格好，对每个状态取大的启发式函数

+ 可以通过W调节启发式函数的权重

  + W=0，UCS
  + W=$$\infin$$，贪心启发式搜索

#### 2.3.2 A*图搜索 A∗ Graph Search

![image-20250612133847730](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612133847730.png)

+ 启发式**一致性**：启发式边长要小于真实边长

+ $$
  h(n)-h(n')\le c(n,a,n') 如果c是一条真实的路径
  $$

+ 一致性是强条件，所以认为**如果启发式函数是一致的，则它是可接受的**

##### A*图搜索最优性证明（一致性）

1. 一致性保证了任何路径上的f(n)值是非递减的，对于$$n\to n'$$

$$
f(n')=g(n')+h(n')=g(n)+c(n,a,n')+h(n')\ge g(n)+h(n)=f(n)
$$

2. 每当 A∗ 选择任何节点 n 进行扩展时，到该节点的最优路径已被找到
   + 如果存在更优路径，则在路径上存在 n 的前驱节点 p 使 f(p) 的值比 f(n)更大（否则会拓展p而不拓展n），但一致性禁止这一点
3. 选择拓展的第一个目标节点必然是最优解，因为此时的g(n)（最短路径）和h(n)（已经是0）都最小

### 2.4 约束满足问题 Constraint Satisfaction Problems

+ **约束满足问题**（CSP）
  + 状态是结构化的：对于每一个变量域$$X_i$$，有定义域$$D_i$$中定义的变量值
  + 目标测试是一组约束，指定了变量子集的可允许值组合
+ 约束满足问题的模型$$P=(X,D,C)$$
  + **变量**：$$X=\{X_1,X_2,\dots,X_n\}$$
  + **定义域**：$$D=\{D_1,D_2,\dots,D_n\}$$
    + 对于不同变量定义域提供了一个元组
  + **约束**：$$C={p_1,p_2,\dots,p_n}$$，规定了变量值的可接受组合
  + **赋值**：每个变量（注意不是所有变量）都有一个非空值，使得所有值满足约束：$${X_i=v_i,X_j=v_j,\dots}$$
  + **解**：一个约束满足问题的解是一个一致且完整的赋值
+ 约束图
  + 二元约束满足问题：每个约束最多涉及两个变量
  + 二元约束图：节点是变量，弧表示约束

### 2.5 回溯搜索 Backtracking Search

+ 回溯搜索模型
  + **状态**：一组所有变量和对应的赋值（空值也是一种赋值）
  + **初态**：$$\{\}$$，所有变量都是空值
  + **后继函数**（Successor function）：为变量添加一个赋值
  + **目标测试**：所有变量都有非空的赋值且满足约束
+ 回溯搜索基本思想
  1. 解满足交换律，所以可以人为确定一种给变量赋值的顺序，即**变量排序**（variable-ordering）；
  2. 对于一个满足约束的解，其局部解也一定满足约束，所以需要每一步**检查**是否满足约束，并进行**回溯**（fail on violation）
  3. 由于所有的变量都要有赋值，约束满足问题的解一定位于最深处，所以使用深度优先搜索

![image-20250612143034146](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612143034146.png)

+ 回溯搜索的启发式操作
  + **排序**（Ordering）：如何进行变量排序？如何确定赋值的尝试顺序？
  + **过滤**（Filtering）：如何及早过滤很可能失败的路径？
  + **结构**：能否利用问题的结构（进行剪枝或分解）？

#### 2.5.1 排序 Ordering

+ 变量排序：优先选**最受约束的变量**（most constrained variable）
+ 赋值排序：如果要给变量赋值，优先选（使得剩余变量）赋值**自由度最大的值**（least constrained value）

![image-20250612143707576](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612143707576.png)

#### 2.5.2 过滤 Filtering

+ **搜索中过滤**（Search while filtering）：提前维护剩余变量的值域，过滤不合法的赋值
+ **一步前向搜索**（one-step lookahead）：每一步赋值更新**相邻**（1步）剩余变量的值域，如果出现空值域，撤销非法赋值

![image-20250612143912441](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612143912441.png)

##### 弧相容

+ **弧**$$X_i\to X_j$$存在仅当二元约束$$C_{ij}$$生效

  + 弧$$X_i\to X_j$$相容仅当对于$$\forall x_i \in D_i,\exists x_j \in D_j, (X_i=x_i,X_j=x_j)\in C_{ij}$$
  + 如果$$\exists x_i \in D_i$$使弧不相容，从$$D_i$$中删除$$x_i$$
  + **约束传播**：从$$D_i$$中删除$$x_i$$之后，对于$$\forall X_k \to X_i$$，再检查一遍

  ![image-20250612145453996](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612145453996.png)

  + 例子
  + 1. NT删B$$\to$$Q、SA中删G
    2. Q、SA中删G$$\to$$Q、V中删B
    3. Q中删B$$\to$$NSW中删R
    4. 最终NSW可以填B,R，V可以填G,R，这里取其中一种解

##### AC-3

![image-20250612150046252](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612150046252.png)

1. 所有弧入队
2. $$(X_i\to X_j)$$出队，
   1. 对$$D_i$$做revise，如果在$$D_j$$中找不到配对，删掉$$D_i$$中的值
   2. 如果删掉了$$D_i$$中的值，执行传播，将$$\forall X_k\to X_i$$入队
3. 循环直到队空

+ 算法复杂度：对每条弧c，每个取值d都做强制弧相容算法，复杂度是$$O(cd^3)$$
+ 强制弧相容可以**显著缩小域**，但不能保证把域大小缩小到1，在域具有多个取值时，还是应该使用搜索方法

#### 2.5.3 结构 Structure \#

+ **独立子问题**：极端情况，问题的一部分和另一部分之间不存在依赖
+ **树状约束图**：将约束图规约/近似成树状结构，可以将复杂度降为$$O(cd^2)$$

![image-20250612154127358](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612154127358.png)

### 2.6 局部搜索 Local Search

+ CSP问题一般是NP难问题，其解的搜索是指数复杂度的
+ 对于过度复杂的问题，一般使用**局部搜索**找到一个不完备的次优解
+ 局部搜索：每次进行单步操作直到无法变得更好
  + 随机赋一组初始值
  + 改变变量的值使其满足约束
  + 无边缘集

#### 2.6.1 登山法 Hill Climbing

+ 登山法/贪心局部搜索
  + 核心：随机开始；选取一个value最大的邻居节点；不断重复直到无法提升value
    + 约束问题中value可以设置为满足约束的数量
  + 问题：
    + **局部极值**（local optima）：落在局部极值后，无法向邻居方向移动
    + **高原**（plateaus）：落在value相同的节点集后，会迷路，无法获取移动方向
  + 策略：
    + 达到局部最大值时，使用随机步长向随机方向移动
    + 随机选取一组初态，直到有初态通过登山法成功到达解
      + 8-queens问题的期望成功率为0.14，因此大概需要选取7个初态

#### 2.6.2 模拟退火 Simulated Annealing

![image-20250612155940599](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612155940599.png)

+ 策略：按照一种概率（温度）向value略差的邻居移动
+ 模拟退火中的重要参数是**温度**，温度使用**冷却**（cooling）常数计算得出，常见的函数包括
  + 指数/几何：$$T(k)=T_0(\alpha_{k})$$
  + 线性：$$T(k)=T_0-\alpha(k)$$
  + Lundy-Mees：$$T_{k+1}=\frac{T_k}{1+\alpha T_{k}}$$
  + Cauchy：$$T(k)=\frac{T_0}{1+k}$$
  + 其他不使用冷却的方法
+ 冷却值一般是$$0.7-0.9$$

#### 2.6.3 束搜索 Beam Search

+ 策略：每一次迭代考虑最好的k个后继
+ 复杂度：$$O(n(kb)\log(kb))$$，b为分支数
+ 随机束搜索（Stochastic Beam Search）：不一定取最大的k个，而根据value选取一个空间

#### 2.6.4 遗传算法 \#

+ 遗传算法
  + 每一条状态由一个染色体表达
  + 对于每一条染色体，存在一个适应性函数衡量状态是不是好解
  + 后代染色体由亲代染色体通过繁殖、交叉互换和基因突变得到

![image-20250612161431958](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612161431958.png)

### 2.7 对抗搜索 Adversarial Search

+ **博弈**（Gaming）：多于一个智能体参与的任务空间
  + 随机还是确定？（Deterministic or stochastic）
  + 信息是否完备？（Perfect information）
  + 是否存在博弈？（Two, or more players）
  + 是否零和？（Zero sum）
+ 零和博弈（Zero-Sum Games）
  + 智能体存在相反的目标
  + 当一方利益最大化时，另一方利益最小化
+ 一般博弈（General Games）
  + 每个智能体具有独立的目标
  + 智能体可以进行合作、竞争、结盟等行为
+ **标准博弈**（Standard Games）：确定性的、按轮次进行的、具有两个智能体的、信息完备的零和博弈
  + 初态：$$s_0$$
  + 玩家（players）：Player(s) 决定哪个智能体进行动作
  + 动作：Action(s)
  + 转移模型：Result(s,a)
  + 目标测试/终局测试（Terminal Test）：Terminal-Test(s)
  + 效用函数（Utility function）：Utility(s,p) 是玩家 p 在状态 s 下衡量做出决价值的函数
+ **搜索树**：
  + 对任意状态可以定义价值函数$$V(s)$$
  + 对非终节点，价值函数是子节点价值函数的最大值$$V(s)=\underset{a\in \operatorname{Actions}(s)}{\max} V(\operatorname{Result}(s,a))$$
  + 对终局，价值函数等于效用函数$$V(s)=\operatorname{Utility}(s)$$
+ 零和博弈的决策树是逐层交替的搜索树

#### 2.7.1 最小最大算法 Minimax Algorithm

![image-20250612183249615](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612183249615.png)

+ 对非终节点，分层取子节点价值函数的最小/最大值
+ 对终局，仍然以效用函数作为价值函数
+ Minimax算法只能采用深度优先搜索递归求解，因为深度优先搜索能尽可能快地到达终局，从而向根节点传播价值

![image-20250612183701270](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612183701270.png)、

+ Minimax算法具有最优性，如果对手采取最优策略，智能体也必须采取最右策略
+ 当参与博弈的智能体多于2个时：
  + 每个智能体维护自己的价值函数
  + 每个智能体在轮到自己的节点上遍历子节点，对自己的价值函数取最大，对别人的价值函数取最小
+ Minimax的复杂度和DFS相同，时间复杂度是$$O(b^m)$$，空间复杂度是$$O(bm)$$
  + 对于象棋，一般情况下b=35，m=100

#### 2.7.2 A-B剪枝 Alpha-Beta Pruning*

![image-20250612184634810](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612184634810.png)

+ 核心思想：$$\alpha$$表示max节点保证价值的下界，$$\beta$$表示min节点控制价值的上界，对于正常对局上的节点，如果状态的价值是N，一定有$$\alpha<N<\beta$$，如果出现其他的异常情况（比如例子中的$$\beta<\alpha$$），直接剪枝以降低搜索成本。

![image-20250612190136651](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612190136651.png)

+ 在例子中，发生的步骤如下
  1. MAX节点DFS到左侧子树，此时$$\alpha=0,\beta=0$$
  2. 左子树MIN节点DFS到终局，并遍历终局，此时$$\alpha=0,\beta=min(3,12,8)=3$$
  3. MIN节点递归回传，此时$$\alpha=max(0,3)=3,\beta=3$$
  4. MAX节点DFS到中间子树，此时$$\alpha=3,\beta=3$$
  5. 左子树MIN节点DFS到终局，此时$$\alpha=3,v_\beta=min(3,2)=2,v_\beta<\alpha$$，剪枝，向MAX节点回传$$v=2$$（**这表示MAX节点一定不会选择中间子树**）
  6. MAX节点持有$$\alpha=3,\beta=3$$，和$$v=2$$比较，无需改动，于是DFS到右侧子树
+ 在例子中，由于右侧子树子节点排列顺序不好，价值为14和5终局无法被剪枝
+ 因为剪枝的节点根本不会被选中出现对局，所以剪枝对minimax计算无影响

![image-20250614130408513](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614130408513.png)

（a）求状态值

发生的步骤如下：

1. MAX节点DFS到左侧子树，此时$$\alpha=0,\beta=0$$

2. 左子树MIN节点DFS到终局10,8,3，并遍历终局，此时$$\alpha=0,\beta=min(10,8,3)=3$$

3. 左子树MIN节点递归回传，此时MAX$$\alpha=max(0,3)=3,\beta=3$$4。 

4. MAX节点DFS到中间子树，此时$$\alpha=3,\beta=3$$

5. 中间子树MIN节点DFS到终局2，此时$$\alpha=3,v_\beta=min(3,2)=2,v_\beta<\alpha$$，**剪枝**15,7，向MAX节点回传$$v=2$$

6. MAX节点持有$$\alpha=3,\beta=3$$，和$$v=2$$比较，无需改动，于是DFS到右侧子树

7. 左子树MIN节点DFS到终局6,5,4，并遍历终局，此时$$\alpha=max(4,3)=4,\beta=min(6,5,4)=4$$

8. 右子树MIN节点回传，此时MAX$$\alpha=max(4,3)=4,\beta=3$$

9. 状态值MAX节点取MAX，MIN节点取MIN，依次为**4,3,2,4**，分别在第8,3,5,7步更新
10. MAX比较各节点传来的$$\alpha$$值，右子树MIN节点最大，最优决策分枝为**右**

（b）剪枝：在第5步中剪枝**15,7**

##### A-B剪枝的复杂度

+ 好情况：所有节点都按顺序排，每两层展开$$2b$$个节点，总复杂度问为$$O(b^{\frac{d}{2}})$$
+ 随机情况：不知道是$$0.75b^d$$还是$$b^{0.75d}$$，都算不出来
+ 坏情况：同DFS

#### 2.7.3 实时决策 Real-Time Decisions

+ 问题：实际博弈终局太深或者b分支数太大

+ 策略

  + 剪枝（见上一节）
  + 有限前向搜索
    + **截断搜索**（cutting off search）限制搜索深度，或者**前向剪枝**（forward pruning）限制搜索宽度
    + 使用**评估函数**（evaluation function）估计非终局的价值

+ 评估函数：每个训练特征评估函数的总合

+ $$
  \operatorname{Eval}(s)=w_1f_1(s)+w_2f_2(s)+\dots +w_nf_n(s)
  $$

  + 例如对国际象棋，评估函数`=盘面棋子价值+活动性+王安全性+中心控制`

##### 截断搜索

![image-20250612193910354](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612193910354.png)

+ 在限定的深度外，使用评估函数的值替代效用函数，并停止搜索
+ 所有玩家走完一轮称为一层（ply），在最后一名玩家走完后降低深度d
+ 截断搜索通常是不完美的贪心方案

##### 静态搜索

+ 只在**静止位置**（quiescent positions），即特征值变化不大的位置停止搜索
+ 如果状态特征会发生很大的变化，继续搜索一层
+ 加速策略：搜索表、开局表、残局表

### 2.8 蒙特卡洛树搜索 Monte Carlo Tree Search *

+ 适用范围：大深度、高分枝度、无评估函数的博弈
+ **评估函数**：对子树上的所有模拟情况取均值
+ **选择策略**（rollout）：在模拟中选择动作的策略
  + **利用**（Exploitation）：优先采取确定性更强的动作
  + **探索**（Exploration）：优先采取更不确定的动作

#### 2.8.1 置信上界 Upper Confidence Bound

对于父节点 $$p$$ 的某个子节点 $$i$$:
$$
\operatorname{UCT}(i)=v_i+C\times \sqrt{\frac{\ln(N)}{n_i}}
$$

| 符号    | 解释                                     |
| ------- | ---------------------------------------- |
| $$v_i$$ | 节点$$i$$的平均回报（回报次数/访问次数） |
| $$n_i$$ | 节点$$i$$的访问次数（i的子树展开次数）   |
| $$N$$   | 节点p的访问次数（p的子树展开次数）       |
| C       | 置信上界参数（探索的权重）               |

+ 对于UCB可以推导出UCT算法

![image-20250612200312646](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612200312646.png)

#### 2.8.2 UCT*

##### MCTS: 选择 selection

![image-20250612200426263](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612200426263.png)

+ 选择策略：UCB选择策略，假设$$U(s)$$是本节点s模拟的胜局数

+ $$
  \operatorname{UCB1}(n)=\frac{U(s)}{N(s)}+C\times\sqrt{\frac{\log N(\operatorname{Parent}(s))}{N(s)}}
  $$

  ![image-20250612200715676](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612200715676.png)

##### MCTS：拓展 expansion

+ 随机选择一个未访问的子节点位置，并将一个新的记录节点添加到统计树中
+ 和选择的本质区别：拓展决定将哪些子节点放进树，选择决定从哪一个子节点继续深入

+ ![image-20250612200835434](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612200835434.png)

##### MCTS：模拟 simulation

+ 使用随机策略对节点进行模拟，直到终局

##### MCTS：回传 Backpropagation

+ 对路径上的所有祖先节点进行U和N的更新

![image-20250612201453965](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612201453965.png)

##### 为什么MCTS？

+ 优点
  + 生成**不对称树**，平衡拓展和探索
  + **不受分支因子**的影响（对于任意分支因子，都可以设置模拟次数）
  + 容易适应新游戏（不需要预生成价值函数）
  + 不需要启发式，但可以集成（比如直接在UCB添加一个启发式项）
  + 随时算法，可以按需完成
  + 稍微可并行化
+ 缺点
  + 无法模拟极端深度
  + 需要便于模拟的大量计算资源
  + 依赖于“弱相关”的随机游戏
  + 许多变体需要专业知识进行调整
  + 理论不可解释

#### 2.8.3 PUCT **

$$
\operatorname{PUCB1}(n)=\frac{U(s)}{N(s)}+C_{puct}\times P_i\times{\frac{\sqrt{ N(\operatorname{Parent}(s))}}{1+N(s)}}
$$

+ 嵌入了一个启发式/先验概率

## 3. 学习

### 3.1 机器学习 Machine Learning

+ **机器学习**是一个研究领域，它赋予计算机从数据中学习的能力，而不需要被显式编程。

#### 3.1.1 框架 Framework

+ 学习问题的组成

| 符号              | 名称            | 意义                                           |
| ----------------- | --------------- | ---------------------------------------------- |
| $$x\in \R^d$$     | Input           | 输入，数据的特征向量                           |
| $$y=\{0,1\}$$     | Output          | 输出，为标签                                   |
| $$f:x\to y$$      | Target function | 未知的目标函数：从输入到输出的映射             |
| $$\{(x_i,y_i)\}$$ | Data            | 数据：输入到输出的已标记映射                   |
| $$h:x\to y$$      | Hypothesis      | 假设：机器学习从样本训练出的接近目标函数的函数 |

+ **假设空间**（Hypothesis Space）：假设函数存在的范围：$$\mathcal{H} = {h}, h \in \mathcal{H}$$
  + 我们希望假设空间具有以下优良性质，即**正则性**（regularity）
    + **连续性**（Continuity）
    + **光滑性**（Smoothness）
    + **简单性**（Simplicity）
+ **学习算法**：寻找最好h的算法$$\mathcal{A}$$
+ 学习模型：

![image-20250612210043745](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612210043745.png)

+ **损失函数**（Loss Function）：衡量假设值和真实值误差的函数

$$
\mathcal{l}(y,h(\vec{x}))=(y-h(\vec{x}))^2\ (\operatorname{Regression})
$$

$$
\mathcal{l}(y,h(x))=\bold{1}[y\ne h(\vec{x})](\operatorname{Classification})
$$

+ **经验误差/训练误差函数**（canonical training loss function）：求取参数$$\theta$$所有样本的损失函数之和最小
  + 其中$$\theta$$是参数化假设$$h$$的参数

$$
\hat{\epsilon}(h)=\underset{\theta}{\min}\sum_{i=1}^{m}\mathcal{l}(h_{\theta}(x_i),y_i)
$$

![image-20250612211818557](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612211818557.png)

+ **监督学习**（Supervised Learning）：对于每条$$(h(x),y)$$，存在已知的y
+ **无监督学习**（Unsupervised Learning）：对于每条$$(h(x),y)$$，y未知
+ **强化学习**（Reinforcement Learning）：对于状态转移给予奖励信号，智能体相获得奖励最大的方向执行动作
  + 预训练和大数据学习一般是无监督学习
  + 后训练和微调一般是监督学习
  + 推理增强一般是强化学习

#### 3.1.2 估计矩阵 Evaluation Metrics

##### 混淆矩阵 Confusion Matrix

![image-20250612212357326](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612212357326.png)

| 名称        |                | 含义                       | 计算                          |
| ----------- | -------------- | -------------------------- | ----------------------------- |
| Precision   | 精确率/查准率  | 阳性结果中正确结果占比     | $$\frac{TP}{TP+FP}$$          |
| Recall      | 召回率/查全率  | 正确结果中阳性结果占比     | $$\frac{TP}{TP+FN}$$          |
| Accuracy    | 准确率         | 所有结果中查询准确结果占比 | $$\frac{TP+TN}{TP+FP+TN+FN}$$ |
| Error       | 错误率         | 所有结果中查询错误结果占比 | $$\frac{FP+FN}{TP+FP+TN+FN}$$ |
| TPR         | 正确结果阳性率 | 正确结果中阳性结果占比     | $$\frac{TP}{TP+FN}$$          |
| FPR         | 错误结果阳性率 | 错误结果中阳性结果占比     | $$\frac{FP}{FP+TN}$$          |
| Sensitivity | 敏感度         | $$P(\hat{Y}=1|Y=1)$$       | =Recall,TPR                   |
| Specificity | 特性度         | $$P(\hat{Y}=0|Y=0)$$       | =(1-FPR)                      |

+ **接受者操作特性曲线**（Receiver Operating Characteristic, ROC）：横坐标为FPR，纵坐标为TPR的曲线，越接近左上角（TPR=1,FPR=0）越好
+ **曲线下方面积**（Area Under the Curve, AUC）：ROC曲线下方的面积，AUC越接近1模型区分正负样本的能力越强

##### 误差 Error

+ **误差**是前向传播中产生的，反向传播中取梯度进行优化的是损失

| 名称           | 方法                      | 计算                                  | 性质                                    |
| -------------- | ------------------------- | ------------------------------------- | --------------------------------------- |
| 均方误差       | Squared Error             | $$(y_i-\hat{y_i})^2$$                 | 对噪声和极端值非常敏感                  |
| 绝对值误差     | Absolute Error            | $$|y_i-\hat{y_i}|$$                   | 对应于$$L_1$$损失的期望值               |
| 平方对数误差   | Squared Logarithmic Error | $$(\log(1+y_i)-\log(1+\hat{y_i}))^2$$ | 适用于按指数分布的数据                  |
| 绝对百分比误差 | Absolute Percentage Error | $$|\frac{y_i-\hat{y_i}}{y_i}|$$       | 可以区分极端值；不适用于$$y_i=0$$的情况 |

#### 3.1.3 模型选择 Model Selection

+ **贝叶斯错误率**：真实数据对应的决策面的错误率（反映了噪声和随机性）
  + 只选择尽可能好的模型
+ 双集学习
  + **训练集**：训练假设h的参数$$\theta$$
  + **测试集**：评估模型的效果并选择最好的模型
    + 可能在测试集上发生过拟合
    + 学习算法不应该获取测试集内容
+ 三集学习
  + 训练集：训练假设h的参数$$\theta$$，可调参
  + **验证集**（Validation）：评估模型的效果，可验证可调参
  + 测试集：评估模型，不能调参
    + A-B检验：新模型超过现有模型再上线
+ **交叉验证**（Cross Validation）：从总数据集（T+V）中交叉划分训练集和验证集，对验证结果取平均
  + 验证集占总数据集的$$1/n$$，就是**n-折交叉验证**（n-fold cross validation）
+ 模型选择流程

![image-20250612220217143](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612220217143.png)

+ 这一段流程的总复杂度是`算法空间大小*超参数空间大小*验证次数*学习算法复杂度`，即

$$
C_{learning}=\sum_{A^{(i)}\in \mathcal{A}}|\Lambda^{(i)}|\cdot K\cdot O(A^{(i)})
$$

![image-20250612220545054](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250612220545054.png)

### 3.2 线性回归 Linear Regression

+ 线性函数族是最简单的函数族，是最正则的假设空间
+ 对于要进行线性回归的数据，一般分为以下几个部分
  + 主键：区分样本的属性（比如时序），一般不用作特征
  + **样本空间**（Instance Space）：特征向量所在的空间$$\mathcal{X}=\R^d$$
  + **标签空间**（Label Space）：输出标签所在的空间$$\mathcal{Y}=\R$$
+ 线性回归的假设空间

1. 假设训练数据为$$\{(\vec{x}_i,y_i)\},\vec{x}\in\R^d,y\in \R$$

2. 在向量x上加上线性表示截距的哑元1：$$\vec{x}=(1,x_1,\dots,x_d)\in \R^{d+1}$$

3. $$
   h(x)=w_0+\sum_{j=1}^{d}w_jx_j=\sum_{j=0}^{d}w_jx_j=\vec{w}\cdot\vec{x}
   $$

   + $$\vec{w}$$是超平面上的法向量

+ 线性回归的训练损失：对样本到超平面的法向距离做平方误差

$$
\hat{\epsilon}(h)=\sum_{i=1}^{n}(h(x_i)-y_i)^2
$$

+ 线性回归的训练目标：训练出损失最小的法向量$$\vec{w}$$

$$
\hat{\epsilon}(h,w)=\underset{\vec{w}}{\min}\sum_{i=1}^{n}(h_{\vec{w}}(x_i)-y_i)^2
$$

#### 3.2.1 优化 Optimization

+ 平方误差显然是一系列抛物线的和，因此loss曲线是一个**凸的光滑曲面**，沿梯度下降方向一定能达到极值

##### 梯度下降法 Gradient Descent *

这一段PPT的参数意义不明，对于误差/损失，为了方便，假设损失函数J直接用平方误差函数（实际情况一般改），设这个损失包含m条n+1维的样本：
$$
h_w(\vec{x})=w_0+w_1x_1+\dots+w_nx_n
$$

$$
J(\vec{w})=\sum_{i=1}^{m}(h(x_i)-y_i)^2
$$

然后我们求出损失的梯度：
$$
\vec{g}=\nabla_{\vec{w}}J(\vec{w})
$$

$$
g_j=\nabla_{w_j}J(\vec{w})=\frac{\partial J}{\partial w_j}=2\sum_{i=1}^{m}(h_w(x_i)-y_i)x_j
$$

现在我们求出了梯度，需要考虑加在什么地方。我们发现一阶泰勒展开式有梯度的一次项：
$$
J(\vec{w})=J(\vec{w_0})+(\vec{w}-\vec{w_0})^{T}J^1=J(\vec{w_0})+(\vec{w}-\vec{w_0})^{T}\vec{g}
$$
所以对于每一次迭代t，沿梯度下降的方向走一个小步长：
$$
J(\vec{w}-\eta\vec{g})\approx J(\vec{w})-\eta\vec{g}^T\vec{g}
$$

$$
\vec{w}^{t+1}\leftarrow \vec{w}^t - \eta\nabla_{\vec{w}}J(\vec{w})=\vec{w}^t - \eta\nabla_{\vec{w}}\hat\epsilon(\vec{w})
$$

对于线性回归来说，X是n+1列m行的矩阵
$$
\nabla_{\vec{w}}\hat\epsilon(\vec{w})=2X^T(X\vec{w}-\vec{y})
$$
所以总的移动是：
$$
\vec{w}^{t+1}\leftarrow\vec{w}^t-2\eta X^T(X\vec{w}^t-\vec{y})
$$
请检查以上的推导是否正确。

设总共要迭代T次，理论收敛速度是$$O(1/\sqrt{T})$$，复杂度是$$O(dnT)$$

##### 随机梯度下降 SGD

+ 假设每次移动的方向在梯度下降方向上进行偏离操作

![image-20250613124724625](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613124724625.png)

#### 3.2.2 非线性化 Nonlinearization

+ 线性模型只能拟合形如$$y\approx \vec{w}^T\vec{x}+b$$的关系
+ 如果数据显然非线性，可以将输入x映射到更高维度的空间

$$
\Phi:\R^d\longrightarrow\R^D,\vec{z}=\Phi(\vec{x}),\longrightarrow y=\vec{w}^T{\vec{z}}+b
$$

+ 映射$$\Phi$$的每个分量都是一个**基函数**（Basis function）
+ 基函数可以在统计学习中学出来，最常用的基函数是

| 基函数族 | 例子                                                         | 超参数（空间参数）                  | 场景                     |
| -------- | ------------------------------------------------------------ | ----------------------------------- | ------------------------ |
| 多项式   | $$\Phi(x_1,x_2)=\{1,x_2,x_1x_2,x_1^2\}$$                     | 最大阶数q                           | 低维（比如抛物线分布）   |
| 径向基   | $$\Phi_j(\vec{x})=\exp{(-\frac{||\vec{x}-\mu_j||_2^2}{2\sigma_j^2})}$$ | 高斯中心$$\mu$$，高斯宽度$$\sigma$$ | 局部性强（比如聚集分布） |
| 三角     | $$\Phi_j(\vec{x})={\cos(2\pi f_kx)\sin(2\pi f_k x)}$$        | 频率$$f_k$$                         | 周期（比如时序数据）     |

#### 3.2.3 正则化 Regularization

![image-20250613130350988](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613130350988.png)

+ 如果$$\vec{w}$$太复杂，可能会导致过拟合，因此必须通过正则化限制$$\vec{w}$$的复杂程度

##### 2-范数正则化 L2-Regularization

$$
\hat{\epsilon}(h,w)=\underset{\vec{w}}{\min}\sum_{i=1}^{n}(h_{\vec{w}}(x_i)-y_i)^2\to \underset{\vec{w}}{\arg \min}\sum_{i=1}^{n}(W^T(x_i)-y_i)^2 + \lambda||\vec{w}||_2^2
$$

$$
\lambda||\vec{w}||_2^2=\lambda(\sum_{i=1}^{N}(w_i)^2)
$$

##### 1-范数正则化 L1-Regularization

$$
\hat{\epsilon}(h,w)=\underset{\vec{w}}{\min}\sum_{i=1}^{n}(h_{\vec{w}}(x_i)-y_i)^2\to \underset{\vec{w}}{\arg \min}\sum_{i=1}^{n}(W^T(x_i)-y_i)^2 + \lambda||\vec{w}||_1
$$

$$
\lambda||\vec{w}||_2^2=\lambda(\sum_{i=1}^{N}|w_i|)
$$

+ 2-范数正则化正则区域是光滑的，而1-范数正则化正则区域有很多非连续极值点（类比菱形和圆形）的，因此1-范数正则化的解是**稀疏的**
  + 稀疏的解可以简化模型，提取关键特征并增加可解释性

##### $$\lambda$$的调整

+ 过拟合：$$\lambda$$增大
+ 欠拟合：
  + 如果$$\lambda$$较大，适当减小$$\lambda$$
  + 如果$$\lambda$$已经很小，再添加一些基函数

$$
\hat{\epsilon}(h,w)=\underset{\vec{w}}{\min}\sum_{i=1}^{n}(h_{\vec{w}}(x_i)-y_i)^2\to \underset{\vec{w}}{\arg \min}\sum_{i=1}^{n}(W^T(x_i)-y_i)^2 + \lambda\Omega{(\vec{w})}
$$

### 3.3 线性分类  Linear Classification

+ 一般情况下假设函数$$h(\vec{w})$$输出的是实数，需要使用离散化方法将实数映射到形如$$\{0,1\}$$的整数

#### 3.3.1 逻辑斯特回归 Logistic Regression*

+ 首先做一个简单的假设，如果$$h(x)\le 0$$，$$\sigma(h(x))=1$$，否则$$\sigma(h(x))=0$$
  + 这个函数显然不连续
+ 然后采取平滑的方案，希望输出标签1的概率而不是直接输出1，假定$$P(y=1|h(x)=\infin)=1,P(y=1|h(x)=-\infin)=0,P(y=1|h(x)=0)=0.5$$

##### 阶跃函数 Sigmoid function

+ 对于上面的平滑的方案，可以构造出以下的最大似然估计函数：

$$
\log{\frac{P(y=1|\vec{x})}{P(y=0|\vec{x})}}=h(\vec{x})=\vec{w}^T\vec{x}+b
$$

$$
P(y=1|\vec{x})=\frac{\exp(h(\vec{x}))}{1+\exp(h(\vec{x}))}=\sigma(h(\vec{x}))={\color{red}\frac{1}{1+e^{-h(\vec{x})}}}
$$

+ 这样就得到了**sigmoid函数**
+ 如果把指数项改为$$e^{-ah(\vec{x})}$$，就是带温度的sigmoid函数，温度越高越接近线性化

##### 交叉熵损失函数 Cross-Entropy Loss

+ 既然输出了一个概率，自然需要对概率的损失函数

我们假设正确的二元离散分布概率是Bernoulli分布：
$$
P(y_i|x_i)=\sigma(h(x_i))^{y_i}(1-\sigma(h(x_i)))^{1-y_i}
$$
取log可得
$$
-\mathcal{L}(1)=y_i\log(\sigma(h(x_i)))+(1-y_i)\log(1-\sigma(h(x_i)))
$$
直接把y=1，y=0代入上面的式子，显而易见**交叉熵损失函数**是：
$$
\mathcal{L}= \begin{cases}
  & \text{ if } y=1 ,-1\times \log (\sigma(h))\\
  & \text{ if } y=0,-1\times \log(1-\sigma(h))
\end{cases}
$$
![image-20250613134915805](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613134915805.png)

+ 如果使用逻辑斯特回归，需要替换线性回归中的损失函数

+ 
  $$
  \hat{\epsilon}(h,w)=\underset{\vec{w}}{\min}\sum_{i=1}^{n}(h_{\vec{w}}(x_i)-y_i)^2\\\to \underset{\vec{w}}{\arg \min}\sum_{i=1}^{n}(W^T(x_i)-y_i)^2 + \lambda||\vec{w}||_2^2 \\ \to  \underset{\vec{w}}{\arg \min}-\sum_{i=1}^{n}(y_i\log\sigma(h_w(\vec{x}))+(1-y_i)log[1-\sigma(h_w(x))]) + \lambda||\vec{w}||_2^2
  $$

##### 加权逻辑斯特回归 Locally-weighted Logistic Regression **

（1）加权逻辑斯特回归的负对数似然估计

我们已经得到逻辑斯特回归式子
$$
P(y_i|x_i,\vec{w})=\sigma(h(x_i))^{y_i}(1-\sigma(h(x_i)))^{1-y_i}
$$

$$
-\mathcal{L}(1)=y_i\log(\sigma(h(x_i)))+(1-y_i)\log(1-\sigma(h(x_i)))
$$
对它进行加权：
$$
\mathcal{L}(\vec{w})=-\sum_{i=1}^{N}\alpha_i[y_i\log(\sigma(h(x_{i})))+(1-y_i)\log(1-\sigma(h(x_i)))]
$$

$$
\mathcal{L}(\vec{w})=-\sum_{i=1}^{N}\alpha_i[y_i\log(\frac{e^{z_i}}{1+e^{z_i}})+(1-y_i)\log(1-\frac{e^{z_i}}{1+e^{z_i}})]
$$

$$
\mathcal{L}(\vec{w})=-\sum_{i=1}^{N}\alpha_i[y_i\log(\frac{e^{z_i}}{1+e^{z_i}})+(1-y_i)\log(\frac{1}{1+e^{z_i}})]
$$

$$
\mathcal{L}(\vec{w})=-\sum_{i=1}^{N}\alpha_i[y_i\log({e^{z_i}})+(1)\log(\frac{e^{-z_i}}{1+e^{-z_i}})]
$$

$$
\mathcal{L}(\vec{w})=\sum_{i=1}^{N}\alpha_i[(1-y_i)\log({e^{z_i}})+\log({1+e^{-z_i}})]
$$

最后令$$\mathcal{L}=-logP(y|x,w),z_i=\vec{w}^Tx_i$$，得到题目中要求的形式

（2）梯度下降
$$
\mathcal{L}(𝑤)=−log𝑃(𝑦|𝑥,𝑤)+𝜆Ω(𝑤)
$$
先不考虑正则项和加权，使用经典式子
$$
ℓ_i(\vec{w})=(1-y_i)z_i+\log(1+e^{-z_i})
$$

$$
\nabla_wℓ_i=\frac{\partial ℓ_i}{\partial z_i}\frac{\partial z_i}{\partial {\vec{w}}}= \frac{\partial ℓ_i}{\partial z_i} x_i
\\=[\frac{d\log(1+e^{-z_i})}{dz_i}+\frac{d(1-y_i)z_i}{dz_i}]x_i
\\=[\frac{-e^{-z_i}}{1+e^{-z_i}}+(1-y_i)]x_i
\\=[\sigma(z_i)-1+1-y_i]x_i
\\=[P(y_i|x_i,w)-y_i]x_i
$$

加权后是：
$$
\nabla_wL=\sum_{i=1}^{N}\alpha_i(p_{i}-y_i)x_i+\vec{w}
$$
梯度下降更新式为：
$$
\vec{w}\leftarrow \vec{w} - \eta[X^{T}(\vec{\alpha}\odot(\vec{p}-\vec{y}))+\lambda\vec{w}]
$$
代码是：

```python
# Inputs:
#   X  :  N×d  matrix of samples
#   y  :  N     binary labels {0,1}
#   x0 :  d     query point
#   tau:  RBF bandwidth
#   λ  :  regularisation strength
#   η  :  learning-rate
#   T  :  max iterations
#   tol:  gradient-norm stop threshold
#   l1 :  bool, True → L1  False → L2

# --- step-0: compute sample weights for this x0
a = exp(- ||X - x0||^2 / (2*tau^2))        # shape (N,)

# --- step-1: initialise weights
w = zeros(d)

for t in range(T):

    # forward -----------------------------------------------------------
    z = X @ w                               # shape (N,)
    p = 1 / (1 + exp(-z))                   # sigmoid

    # gradient ----------------------------------------------------------
    g = X.T @ (a * (p - y))                 # weighted data term
    g += λ * w                          # L2 gradient

    # check convergence
    if norm(g) < tol:
        break

    # update ------------------------------------------------------------
    w -= η * g

# probability estimate at the query point
prob = 1 / (1 + exp(-(w @ x0)))

```

（3）加权逻辑斯特回归的退化

+ 权重接近1（RBF带宽极大）：退化到**普通逻辑斯特回归**甚至**线性回归**
+ 权重接近0（RBF带宽极小）：退化到**1-近邻分类**

![image-20250613144259511](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613144259511.png)

### 3.4 决策树 Decision Tree

+ 对于特征非常多的数据集，先采用一些特征进行粗分类，再采用一些特征进行细分类
  + 按特征顺序划分数据集产生的子集树是**决策树**

#### 3.4.1 ID3 算法 ID3 Algorithm*

![image-20250613144902852](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613144902852.png)

+ 节点的划分

  + **平衡**：划分为差不多大小的子集
  + **纯粹**：尽量将不同的类别分别划入不同的子集

+ **分类错误率**

  + 两类分类错误率（classification error）：$$\min (\frac{\mathcal{|C_1|}}{\mathcal{|D|}},\frac{\mathcal{|C_2|}}{\mathcal{|D|}})$$
  + K类分类错误率（k-misclassification error）：$$1-\underset{1\le k\le K}{max}(\frac{\mathcal{C_k}}{D})$$

+ 节点的**熵函数**（Entropy）

  + $$
    H(\mathcal{D})=-\sum_{k=1}^{K}\frac{|\mathcal{C}_k|}{|\mathcal{D}|}\log \frac{|\mathcal{C}_k|}{|\mathcal{D}|}
    $$

  + 前面的负号表示玻尔兹曼熵

+ 节点的**基尼系数**（Gini Index）

  + $$
    Gini(\mathcal{D})=1-\sum_{k=1}^{K}(\frac{|\mathcal{C}_k|}{|\mathcal{D}|})^2
    $$

![image-20250613145708035](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613145708035.png)

+ **信息增益**（Information Gain, IG）：父节点熵减去子节点熵的加权和
  + 熵减小的过程就是分类的过程

$$
{\color{red}H(\mathcal{D_1}\cup\mathcal{D_2})-[\frac{|\mathcal{D_1}|}{|\mathcal{D}|}H(\mathcal{D_1})+\frac{|\mathcal{D_2}|}{|\mathcal{D}|}H(\mathcal{D_2})]}
$$

+ **ID3算法**
  1. 将数据集全集作为根节点
  2. 对每一个特征计算信息增益，选取最好的划分特征A
  3. 根据A的取值划分子树
  4. 进入子树，如果数据集已经纯粹（无论如何划分均没有信息增益），停止划分
+ 复杂度：$$O(dn\times \operatorname{depth})$$

![image-20250613150814703](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613150814703.png)

#### 3.4.2 C4.5 算法 C4.5 Algorithm*

+ 背景：如果误用了类别很多的特征（如主键），划分出的子数据集会异常纯粹

  + 策略：对分组数量进行惩罚

+ **C4.5**算法：计算划分的固有值

  + $$
    \operatorname{IV}(f)=-\sum_{i=1}^{V}\frac{|\mathcal{D}_i|}{|\mathcal{D}|}\log \frac{|\mathcal{D}_i|}{|\mathcal{D}|}
    $$

  + $$
    {\color{red}\operatorname{IG_IV}=\frac{\operatorname{IG}(f)}{\operatorname{IV}(f)}}
    $$

![image-20250613151718198](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613151718198.png)

+ 连续值的分割
  + 先对连续值进行排序（连续值一定是可排序的）
  + 将连续值出现的相邻值的1/2作为划分点（Split Point）的集合
  + 计算信息增益，寻找最好的划分点

#### 3.4.3 理解决策树 Understanding Decision Tree

+ **预剪枝**（Pre-Pruning）和**后剪枝**（Post-Pruning）

| 维度             | **预剪枝**                                                   | **后剪枝**                                                   |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **基本思路**     | 在**生长过程中**就检测节点是否继续划分；若不满足 “收益阈值”→停止分裂 | 先让树**尽量长大**（甚至到纯叶或最小样本数），再自底向上剪除“多余”子树 |
| **常见停止条件** | • 信息增益 / 基尼下降 <br />• 节点样本数 < n<br />• 树深度 > d<br />• χ²/卡方检验不显著 | • **代价-复杂度剪枝**（CART 的 α-pruning）<br />• **误差-减幅剪枝**（C4.5 的 error-based pruning）<br />• 交叉验证估计剪枝后误差 |
| **优点**         | 训练速度快、模型小；易与流式数据结合                         | 可先捕获潜在结构，再统一评估；通常泛化性能更好               |
| **缺点**         | 过早停止 → 高偏差；容易漏掉后续能带来纯度提升的组合划分      | 训练阶段耗时、占内存；需要额外验证集或交叉验证               |
| **何时推荐**     | • 数据量大、实时性要求高<br />• 内存/算力受限场景            | • 追求准确率，数据集规模适中<br />• 离线训练，对资源不敏感   |

+ **树复杂度正则化**（Tree Complexity Regularization）
  + `和（叶节点大小*叶节点熵）+正则化因子*叶节点数量`

$$
\operatorname{Cost}_\alpha(T)=\operatorname{Error}(T)+\alpha|T|
\\=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{t,k}\log\frac{N_{t,k}}{N_{t}}+\alpha|T|
$$

+ 树复杂度剪枝：从叶节点开始，递归地将比父节点更复杂的子树简化成叶子
+ 决策树的拟合能力
  + 决策树的拟合能力和线性模型相比**既强又弱**
  + 在二维/三维特征空间内，决策树使用不一定相连的长方形/立方体空间拟合分类，这种划分是强可解释的，但是是非线性的
  + 决策树的假设函数是**示性函数**（label function）：$$h(\vec{x})=\sum_{i=1}^{m}c_i\vec{1}\{\vec{x}\in R_i\}$$
    + 示性函数的基函数无法通过梯度下降求解
  + 决策树和线性模型哪个模拟能力更强取决于问题和基函数构造

### 3.5 随机森林 Random Forest

#### 3.5.1 装袋 Bagging

+ Bootstrap

  + 从原始训练集$$D_n$$中**有放回**的抽取B份相同规模的样本$$D_n^1,\dots,D_n^B$$
  + 每份中样本不被抽中的概率是$$(1-\frac{1}{n})^n$$，当n很大时被抽中的概率为$$0.632$$
  + 对于**没被装袋数据**（Out-of-Bag Data, OOB），设没被装袋的次数为$$s_i$$，直接作为这些袋的验证集：$$h_{OOB}(x_i)=\frac{1}{|S_i|}\sum_{b\in S_i}h_{n}^{b}(x_i)$$

+ Aggregation

  + 每份样本训练$$h_{n}^{1},\dots,h_{n}^{B}$$，对总包袋使用$$h_{bag}(x)=\operatorname{Combine}(h_n^1(x),\dots,h_{n}^{B}(x))$$方法
    + 回归：学习函数求均值
    + 分类：学习函数结果投票作为最终结果
  + 原理是降低学习函数的方差而不显著增大偏差（推导略）

#### 3.5.2 布雷曼算法 Breiman Algorithm

+ 普通随机森林：使用装袋法获取数据集，在每个数据集上长决策树
  + 缺点：树-树相关性高！如果每棵树都长得差不多怎么办？
+ **布雷曼算法**

![image-20250613155912015](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613155912015.png)

1. 还是采取 Bootstap 装袋法获取子样本集
2. 对d个特征，**随机抽取**$$\sqrt{d}$$（分类）$$\frac{d}{3}$$（回归）个特征长决策树
3. 决策树形成后一直长到全深树为止，2,3步并称为**随机生长**（Randomized tree Building ）

+ 布雷曼算法有效降低了随机森林的相关性，增加了多样性
+ 随机森林的性质
  + **不可约误差**（Irreducible error）：噪声等因素导致的稳定的误差
  + **方差**（Variance）：模型不稳定性导致的误差，随模型的复杂性增大
  + **偏差**（Bias）：模型的拟合能力导致的误差，随模型的复杂性减小
  + 训练的作用是寻找方差+偏差最小的点，即恰好拟合的点
    + 随机森林的主要优势是容易降低方差

![image-20250613161039072](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613161039072.png)

### 3.6 强化学习 Reinforcement Learning

![image-20250613180539274](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613180539274.png)

+ 强化学习问题

  + **目标**（Objective）：状态移动的方向

  + **状态**（State）：对智能体所处环境的描述
  
  + **动作**（Action）：智能体做出的可以改变环境的行为
  
  + **奖励**（Reward）：环境返回给智能体的信号
    + 即时奖励（each time stamp）：（例如Atari Games）
    + 延迟奖励（at the end）：（例如象棋）
  

#### 3.6.1 马尔科夫决策过程 Markov Decision Process*

$$
M=<\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\mathcal{\gamma}>
$$

| 符号            | 名称                   | 形式                                            | 意义                        |
| --------------- | ---------------------- | ----------------------------------------------- | --------------------------- |
| $$\mathcal{S}$$ | states                 | $$s\in \mathcal{S}$$                            | 马尔科夫状态集              |
| $$\mathcal{A}$$ | actions                | $$a\in \mathcal{A}$$                            | 马尔科夫动作集              |
| $$\mathcal{R}$$ | reward                 | $$r=(s,a)$$                                     | 对于状态上动作的奖励信号    |
| $$\mathcal{P}$$ | transition probability | $$P_{sas'}=\mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]$$ | 转移概率分布                |
| $$\gamma$$      | preference             |                                                 | 智能体对短期/长期奖励的偏好 |

+ 马尔科夫决策过程

![image-20250613182437551](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613182437551.png)

+ 马尔科夫决策过程是无先验的决策过程，其样本是在学习中获取的。因此，学习关键是寻找函数衡量状态和策略的优劣

##### 策略 Policy

+ **策略** $$\pi$$ 是 $$\mathcal{S}\to \mathcal{A}$$的映射，决定在某个状态应该采取什么动作
  + **确定性策略**（Deterministic Policy）：$$a=\pi(s)$$，输入状态，输出确定的动作
  + **随机性策略**（Stochastic Policy）：$$\pi(a|s)=P(a|s)$$，输入状态，输出动作的概率分布
+ 目标：找到策略$$\pi^*$$，使得智能体采取该策略形成所有轨迹的期望奖励最大
  + $$\gamma^t$$表示把时间t的奖励按照时间间隔进行指数折扣，这样可以保证无穷马尔科夫路径返回的奖励是有界的。

$$
\pi^*=\arg\max \mathbb{E}[\sum_{t\ge0}\gamma^tr_t|\pi]\\
\operatorname{with} s_0 \sim p(s_0),a_t\sim\pi(\cdot|s_t),s_{t+1}\sim p(\cdot|s_t,a_t)
$$

+ 强化学习和监督学习的概念映射
  + 策略-假设
  + 状态-特征
  + 动作-标签
  + 动作/状态分布-标签/特征分布
+ **状态价值函数**：在策略目标的基础上，添加对初始状态s优劣的衡量
  + 期望部分写成$$\mathbb{E}_{\pi}$$或者其他形式都一样，总之是对奖励求期望

$$
V^{\pi}(s)=\mathbb{E}_{(s_1,a_1),\dots,(s_t,a_t),\dots}[\sum_{t\ge0}\gamma^tr_t|s_0=s,\pi]=\mathbb{E}_{s_1:T,a_1:T}[\sum_{t\ge0}\gamma^tr_t|s_0=s,\pi]
$$

+ **状态-动作价值函数**：在状态价值函数的基础上，分离“当前动作”和“未来动作”

$$
Q^{\pi}(s,a)=\mathbb{E}_{\pi}[\sum_{t\ge0}\gamma^tr_t|s_0=s,a_0=a,\pi]
$$

+ **Bellman方程**：`当前奖励期望=一步转移的奖励+转移后的奖励期望`
  + 策略价值Bellman方程

$$
V^{\pi}(S)_{\not\mathbb{E}}=\sum_{a}\pi(a|s)Q^{\pi}(s,a)
$$

$$
Q^{\pi}(s,a)=r(s,a)+\gamma\mathbb{E}_{s'\sim p(\cdot|s,a)}[V^{\pi}(s')]
$$

$$
{\color{Red} Q^{\pi}(s,a)=\mathbb{E}_{s',a'}[r(s,a)+\gamma Q^{\pi}(s',a')|s,a] }
$$

+ 最优策略Bellman方程

$$
{\color{Red} Q^{*}(s,a)=\mathbb{E}_{s'}[r(s,a)+\gamma \underset{a'}{\max} Q^{*}(s',a')|s,a] }
$$

+ 强化学习方法
  + **基于价值的强化学习**（Value Based RL）：学习最优的价值-状态函数
    + 算出所有状态对应的价值，向高价值状态转移
  + **基于策略的强化学习**（Policy-based RL）：学习最优的策略
    + 学习所有状态对应的最有动作，使用最优动作转移

#### 3.6.2 基于价值的强化学习 Value Based RL*

![image-20250613190839672](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613190839672.png)

+ Q-Learning以Q函数作为要学习的假设函数，对于样本的获取，可以使用MCTS方法或Bellman方法

##### Q-Learning, MCTS

```python
for episode:
    s = env.reset()
    while not done:
        # --- 在模型上跑 MCTS ---
        tree = {}
        for _ in range(M):
            path, reward_sum = mcts_simulate(s, tree, Q)   # UCT, rollout…
            backprop(path, reward_sum, tree)               # N, Q_tree 累加
        # 从树的 visit/N 选动作，或取 Q_tree 最大
        a = best_action(tree.root)
        # 与真实环境交互
        s2, r, done = env.step(a)
        # --- 把树统计融入全局 Q ---
        for (s_t, a_t), q_hat in tree.Q_estimates():
            Q[s_t, a_t] = (1-β) * Q[s_t, a_t] + β * q_hat
        s = s2

```

智能体在进行一次交互前，先做多次MCTS获取样本，再把样本折回Q上，具体为：

1. 对每个(s,a)，先从(s,a)开始与环境交互获取多条路径$$s_{1:T}^i,a_{1:T}^i$$
2. 对每条路径上的每一次(s,a)访问
   1. 增加遍历访问次数：$$N(s,a)\leftarrow N(s,a)+1$$
   2. 添加动作奖励：$$r(s,a)\leftarrow r(s,a)+r_t$$
   3. 使用新的数据折回Q：$$Q(s,a)\leftarrow r(s,a)/N(s,a)$$
3. 使用Q移动到高价值状态，再次MCTS直到所有状态Q都稳定

##### Q-Learning, Temporal Difference

```python
for episode:
    s = env.reset()
    while not done:
        a = ε_greedy(Q[s])
        s2, r, done = env.step(a)
        # --- TD update ---
        Q[s,a] += α * (r + γ * max_a Q[s2,a] - Q[s,a])
        s = s2

```

智能体在每一次交互时，都将真实的r值代入Bellman方程，并对Q函数进行一定的优化：

1. 智能体持有旧的Q函数$$Q_t(s_t,a_t)$$（初始Q函数是**全为0的Q-表**）
2. 智能体进行探索，获取真实的动作奖励r
3. 智能体使用类似SGD（实际上计算机里就是SGD）的方式优化Q函数

$$
l=(r+\gamma\underset{a'}{\max}Q^*(s',a',w)-Q(s,a,w))^2
$$

4. 智能体以一定的学习率将新的Q函数merge到旧的Q函数中

$$
Q_{t+1}(s_t,a_t)\leftarrow Q_t(s_t,a_t)+{\color{red}\alpha}(r_{t+1}+\gamma\underset{a}{\max}Q_t(s_{t+1},a)-Q_t(s_t,a_t))
$$

![image-20250613193354388](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613193354388.png)

+ Q-Learning具有**探索-利用窘境**（Exploration-Exploitation Dilemma）
+ 当使用神经网络训练Q时，没有理论保证

##### Deep Q-Network

![image-20250613193742320](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613193742320.png)

+ 如果使用最新的w更新Q，会影响除了r之外的所有项，相当于对一个移动的函数做SGD，可能做完几轮就找不到函数本身（发散）了
+ 策略：

1. 在每个checkpoint克隆一份w
2. 在checkpoint之间，对于每个$$l=(r+\gamma\underset{a'}{\max}Q^*(s',a',w_{old})-Q(s,a,w))^2$$，使用固定好的$$w_{old}$$定死Loss的前半部分$$r+\gamma\underset{a'}{\max}Q^*(s',a',w_{old})$$
3. 把前半部分视作$$y$$，对$$Q(s,a,w)$$做SGD
4. 新的$$w$$在下一个checkpoint才更新到记忆中

+ **双重DQN**（Double DQN）：使用最新的$$w$$选择动作，使用checkpoint保存的$$w_{old}$$计算奖励
+ **对抗网络**（Dueling network）：将Q-network拆分为状态价值$$V(s,v)$$和动作价值$$A(s,a,w)$$，仅使用最新的$$w$$更新后半部分

#### 3.6.3 基于策略的强化学习 Policy Bases RL*

+ 假设策略函数的参数为$$\theta$$，使用这个参数训练的策略为$$\pi_{\theta}$$；假设$$\tau$$是一条马尔科夫路径（轨迹）
+ 假设使用策略$$\pi_{\theta}$$训练的累计奖励期望为$$J(\tau;\theta)=\mathbb{E}[\sum_{t\ge 0}\gamma^{t}r_t|\pi_{\theta}]$$
+ 需要找到参数最大化累计奖励期望$$\theta^{*}=\arg\max J(\theta)$$

##### 最大似然估计Loss

+ 直接对$$J(\theta)$$做SGD会存在问题，可以将梯度标称在同一个采样分布下的期望

![image-20250613201029341](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613201029341.png)

+ 基于此，在$$J(\theta)$$上使用最大似然估计进行加权
  + 直觉上而言，某条轨迹上的J越大，这条轨迹上的动作a越可能是一个好的动作，应该向这个动作概率更大的方向调节参数$$\theta$$

$$
\arg \max \sum_{t}J(\tau;\theta)\log p(a_t|s_t)
$$

+ 训练过程

![image-20250613201533985](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613201533985.png)

##### 强化学习方式对比

| 角度                | 基于价值                                                     | 基于策略                                                     |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **适用动作空间**    | 离散、小~中                                                  | 连续或高维离散                                               |
| **训练稳定性**      | TD-bootstrap → **偏差小但并不总是有效**(DQN 需 target/memory) | MC/PG → **无偏但方差大**(需 baseline、GAE、信赖域)，训练方式**依赖大量样本** |
| **样本效率**        | 利用 bootstrap，通常较高                                     | 纯 PG 需完整回报，样本效率低（PPO 引入熵/剪切弥补）          |
| **收敛到随机策略?** | 贪婪派生 ⇒ **最终近似确定性**                                | 可输出随机策略（博弈、探索型任务更佳）                       |
| **易陷入局部最优**  | 价值误差大→策略偏移；但一般能全局收敛                        | 直接爬梯度易卡局部极值/平顶；需策略熵、KL 约束帮助           |

+ Q-Learning不保证输出最优策略

##### AC算法  Actor-critic algorithms

+ Critic：获取Q-Learning TD对价值的预测误差，使用该误差维护Q函数的参数w
+ Actor：使用似然估计和Q函数获取策略梯度，使用策略梯度维护$$\pi_{\theta}$$的参数$$\theta$$

![image-20250613202535295](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613202535295.png)

+ AC算法可以使用策略梯度和价值函数迭代逼近最优策略

`P2.4`

> 为什么说相比使用蒙特卡洛（Monte-Carlo）采样的策略梯度（Policy Gradient）方法，Actor-critic 方法的方差更小？

蒙特卡洛对策略梯度的估计是$$∇_{θ}J(θ)=E_{π_{θ}}[∇_{θ}logπ_{θ}(a∣s)⋅R]$$，

而Actor-critic 方法使用的估计是$$∇_{θ}J(θ)=E_{π_{θ}}[∇_{θ}logπ_{θ}(a∣s)⋅(Q(s,a)-V(s))]$$，两者差距一个因子。

实际回报$$R$$受受未来路径上的所有波动影响，因此方差较大；而价值函数的估计$$Q(s,a)-V(s)$$可以在线学习，估计值更稳定、更接近期望，所以Actor-critic 方法的方差更小。（因为MCTS必须一次走到底，而模拟的数据量有限，自然方差就大了）


#### 3.6.4 游戏和现实模型 Games and Real World**

（看作业去）

### 3.7 深度学习 Deep Learning

+ **深度学习**是通过由多个非线性变换组成的架构，对数据中的高级抽象进行建模的算法。
  + 在对数据进行分层抽象的过程中，深度学习算法可以学习到**特征**

#### 3.7.1 多层感知机 Multilayer Perceptron

+ **感知器**（Perception）是一种类似神经元的，基于对输入的特征进行线性加权的分类器
  + 感知器无法表达异或/非线性函数

![image-20250613204047866](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613204047866.png)

+ **多层感知机**（Multilayer Perceptrons，MLP）：由多层感知器组成的可以拟合非线性关系的网络
  + 任意布尔表达式都有唯一多层感知机与之对应
  + 多层感知机使用**隐藏层**（Hidden Layers）进行特征学习，使用**线性层**进行分类器学习；只有隐藏层和线性层计算层数

![image-20250613204359028](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613204359028.png)

+ **激活函数**（Activation Functions）：将非线性输出转化为可SGD输出的函数

![image-20250613205037173](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613205037173.png)

+ **softmax函数**
  $$
  g(\vec{z})_i=\frac{e^{z_i}}{\sum_{j=1}^{k}e^{z_j}}
  $$

  + softmax是“赢者通吃”的函数，使用指数使最大的$$e^z$$在结果中占据主导地位
  + 策略：主导地位太突出$$\to$$ 给$$z$$添加温度函数
  + 策略：会发生数值溢出$$\to$$ 分子分母同除最大的指数项

$$
g(\vec{z})_i=\frac{e^{z_i-z_m}}{\sum_{j=1}^{k}e^{z_j-z_m}},m=\arg\max(z_j)
$$

+ 损失函数

| 名称                                    | 记号 (离散情形)                         | 公式                                                         | 直观含义 / 何时用                                            |
| --------------------------------------- | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **熵**(Shannon entropy)                 | $$H(p)$$                                | $$H(p)= -\sum_{i} p_i\;\log p_i$$                            | **自信息平均量**：分布 p 自己的不确定度在分类任务中，标签的固有熵是一个常数，不随网络参数变化 |
| **相对熵**(Kullback–Leibler Divergence) | $$D_{\mathrm{KL}}\bigl(p\;\|\;q\bigr)$$ | $$D_{\mathrm{KL}}(p\|q)= \sum_{i} p_i\;\log\frac{p_i}{q_i}$$ | **距离/惩罚项**：用分布 p 的权重去衡量它与 q 的差异在蒸馏、变分自编码器、RL 中常出现 |
| **交叉熵**                              | $$H(p,q)$$                              | $$H(p,q)= -\sum_{i} p_i\;\log q_i$$                          | **混合熵**：把“真分布”p中的事件用“预测分布”q去编码时平均需要的信息量深度学习里最常用的 **分类损失**（当 p 是 one-hot 时） |

$$
H(p,q)=H(p)+D_{KL}(p||q)
$$

+ 在深度学习中，最小化交叉熵就是最小化**KL散度**$$D_{KL}$$
+ 深度学习的交叉熵

$$
J(y,\hat{y})=H(y,\hat{y})=-\sum_{j=1}^{k}y_{j}\log\hat{y}_j\\
\min J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[\sum_{j=1}^{k}\vec{1}\{y^{(i)}=j\}\log\frac{\exp(z_{j}^{(n_l)})}{\sum_{j'=1}^{k}\exp(z_{j}^{(n_l)})}]
$$



##### 反向传播（略）

##### 训练策略

+ **优化性质**：达到好的局部极值

+ **泛化性质**：局部极值的周围足够平坦

+ **带冲量的随机梯度下降**（ SGD with Momentum）

  + 冲量是历史路径上累计梯度的加权和

  $$
  \Delta=\beta\Delta+(1-\beta)[\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta,b)]
  $$

  $$
  \theta_{ij}^{(l)}=\theta_{ij}^{(l)}-\eta[\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta,b)]\to\theta_{ij}^{(l)}-\eta\Delta
  $$

+ **学习率衰减策略**（Learning Rate Decay）

  + 逐步减小学习率，便于参数“落到坑底”

  $$
  \eta=\eta_0e^{-kt};\eta=\eta_{0}/(1+kt)
  $$

+ **权重衰减**（Weight Decay）

  + 给权重加上正则项，降低过拟合及数据溢出的概率

  ![image-20250613213235131](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613213235131.png)

+ **随机丢弃**（Dropout）：对每一层的神经元（感知机）随机丢弃50%，可以降低模型的复杂度，避免神经元退化
+ **权重初始化**（Weight Initialization）：不能是太大（溢出）或者太小（连乘后消失）的值，假设d是输入的维度
  + Xavier Initialization：$$\operatorname{Var}(w)=\frac{1}{d}$$
  + He Initialization：$$\operatorname{Var}(w)=\frac{2}{d}$$
    + 由于ReLU网络负值输出无效，需要人工增大方差

![image-20250613213855265](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613213855265.png)

#### 3.7.2 卷积神经网络 Convolutional Neutral Network**

+ 设计思想
  + **局部连接**（Local Connectivity）：每一个神经元只能获取视野中被称为**感受野**（ Receptive field）的局部信息
    + 局部性假设：图片的局部具有识别的空间先验
  + **参数共享**（Parameter Sharing）：神经网络中不同位置的神经元对同一种特征共享参数
    + 平移不变性假设（Shift Invariance Assumption）：特征的效用与所处的空间位置无关
+ **卷积**（Convolution）
  + 使用静止函数f和滑动函数g的内积刻画函数的相似度
  + 卷积值最大的区域就是两个函数相似度最大的区域：卷积核是一个特征，卷积大代表特征出现了

$$
(f*g)(t)=\int_{-\infin}^{\infin}f(\tau)g(t-\tau)d\tau  
$$

`P5.3`  

> 卷积（Convolution）和互相关（Cross-correlation）分别是什么意思？在卷积神经网络中，卷积核通常进行的是卷积还是互相关操作？

卷积的函数定义是：$$(f∗g)[t]= \sum_{τ=−∞}^{∞}f[τ]⋅g[t−τ]$$
互相关的函数定义是：$$(f∗g)[t]= \sum_{τ=−∞}^{∞}f[τ]⋅g[t+τ]$$
在CNN中卷积核一般进行**互相关**操作，因为在CNN中卷积核是学习出来的，没有必要指定卷积核反转。

+ **卷积层**（Convolution Layer）：
  + 图像的重叠维度称为通道
  + 使用卷积核进行卷积提取特征，将每一个特征提取为特征图
  + **步幅**（Stride）用于指定卷积核每次移动的值
  + **填充**（Padding）用于在特征图外部填补0以保证尺寸不变

![image-20250613221753604](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613221753604.png)

![image-20250613221808634](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613221808634.png)

+ **池化层**（Pooling Layer）：减小特征图的尺寸
  + 最大池化：取局部的最大值作为池化值，有一定平移不变性
  + 平均池化：取局部的平均值作为池化值
  + 池化过程中通道数量保持不变
+ **全连接层**（Fully-Connected Layer）：将张量展平为向量

`p5.2.1`

![image-20250613222002051](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613222002051.png)


1. 卷积层有5×5×3=75个可学习参数
2. 最大池化层输入是$$(\frac{55-3}{2})*(\frac{55-3}{2})*3=26×26×3$$，输出是 $$(\frac{26-2}{2}+1)×(\frac{26-2}{2}+1)×3=13×13×3$$
3. 需要进行$$13×13×3=507$$次ReLU函数计算
4. ReLU，Sigmoid，tanh

##### 训练策略

+ 训练神经网络的核心是**数据增广**（Data Augmentation）
+ **批归一化**（Batch Normalization，BN）
  1. 计算批参数：$$\mu,\sigma$$
  2. 对神经元的输出值进行标准化$$\hat{x}=\frac{x-\mu}{\sigma}$$
  3. 学习标准化参数$$y=\gamma\hat{x}+\beta$$

![image-20250613223805816](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613223805816.png)

+ 批归一化可以学习缩放和平移，并轻微正则化

##### 基本结构

![image-20250613223907684](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613223907684.png)

![image-20250613223946812](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613223946812.png)

 #### 3.7.3 注意力机制 Transformer and Attention

+ **语言模型**（language model，LM）：根据已知的词语序列，预测下一个词语的分布
+ **自注意力机制**（Self-Attention）：使用Q,K,V向量对语言模型进行建模
  + 查找过程：Q,K做内积，判断要找哪个上下文
  + 提取过程：查找过程获得的向量和V做外积，把信息和上下文结合

| 名称      | 缩写  | 直观角色                                               | 数学作用                                          | 常见形状 *(单头)*     |
| --------- | ----- | ------------------------------------------------------ | ------------------------------------------------- | --------------------- |
| **Query** | **Q** | “我想找什么？”——当前位置（或待配对对象）发出的检索向量 | 与所有 **K** 做点积，算相似度分数                 | *(批, query_len, dₖ)* |
| **Key**   | **K** | “我是谁？供别人来查找”——序列中每个位置的特征索引       | 被 **Q** 点到后，产生权重 *(注意力权数)*          | *(批, key_len, dₖ)*   |
| **Value** | **V** | “我携带的信息”——真正要被加权汇聚的内容                 | 按 Softmax(Q·Kᵀ/√dₖ) 得到的权重加和，生成最终表示 | *(批, key_len, dᵥ)*   |

##### 变换器 Transformer

+ 模块
  + **可伸缩内积注意力**（Scaled Dot-Product Attention）
  + **多头注意力**（Multi-Head Attention）
  + **位置灵敏FFN**（Position-wise FFN）
  + **残差连接**（Residual Connections）
  + **层规划**（Layer Normalization）
  + **位置编码**（Positional encoding）
+ 结构
  + **仅编码器架构**（Encoder）
  + **掩码解码器架构**（Decoder with Masking）
  + **编码器解码器注意力架构**（Encoder-Decoder Attention）

![image-20250613225317245](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613225317245.png)

##### 可伸缩内积注意力 Scaled Dot-Product Attention

![image-20250613225357030](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613225357030.png)

1. 将向量映射为三组不同的向量，得到Q/K/V矩阵
   $$
   (q_i,k_i,v_i)=(W^q,W^k,W^v)x_i
   $$

2. 使用Q/V矩阵算出向量之间的相似度，并使用softmax处理得到权重
   $$
   \Alpha=\operatorname{Softmax}(\frac{QK^T}{\sqrt{d_k}})
   $$

3. 使用权重对所有向量的v加权求和，得到注意力
   $$
   \operatorname{Attention}(Q,K,V)=\operatorname{Softmax}(\frac{QK^T}{\sqrt{d_k}})V
   $$

4. 使用注意力分配后的y代替原来的x

##### 多头注意力 Multi-Head Attention

+ 将Q/K/V分为多个子空间，在子空间上分别计算注意力

![image-20250613230241492](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613230241492.png)

`P2`

> 本题中我们将探究利用 GPT 类架构进行机器翻译过程中的注意力机制计算过程。
> 假设我们想要翻译“他 | 喜欢 | 苹果”这一中文句子（3 个 token 使用竖线分隔），在 GPT 的某一
自注意力层中 3 个 token 的 query、key、value 向量分别记作 $$Q = {q1, ..., q3}, K = {k1, ..., k3}, V = {v1, ..., v3}, q_{i}, k_{i}, v_{i} ∈ \R^{d}$$
> 1.请写出带掩码的自注意力层
> $$Y=Attention(Q,K,Y)=Softmax(\frac{QK^{T}}{\sqrt{d}})V$$
> 之后每个token对应的输出yi的表达式。（注意以上公式中省略了掩码）

$$yi=\sum_{j}\frac{\exp{(\frac{q_{i}k_{j}}{\sqrt{d}}+M_{ij})}}{\sum_{p}\exp{(\frac{q_{i}k_{p}}{\sqrt{d}}+M_{ip})}}v_{j} \ \ when \begin{cases}
  & \text{ if } i \ge j, M_{ij}=0 \\
  & \text{ else } M_{ij}=-\infty  
\end{cases}$$

> 设 $$d=4,K=\begin{Bmatrix}
\begin{bmatrix}
 0\\
 1\\
 -1\\
0
\end{bmatrix}
\begin{bmatrix}
 1\\
 0\\
 1\\
1
\end{bmatrix}\begin{bmatrix}
 0\\
-1 \\
 0\\
-1
\end{bmatrix}
\end{Bmatrix}$$, 当 GPT 模型预测翻译的第一个 token（英文单
词）“He”的时候，它应该需要尽量多的来自 token“他”的信息。请写出向量 $$q_{3}$$ 的一个取
值，满足 $$q_{3}$$ 的 2 范数不超过 1，且与第一个 token 的自注意力权重（相比别的 token）最大，
并写出此时 $$y_{3}$$ 关于 $$v_{1}$$, $$v_{2}$$, $$v_{3}$$ 的表达式。

不妨令 $$q_{3}=\frac{k_{1}}{||k_{1}||_{2}}=\begin{bmatrix}0 \\ \frac{1}{\sqrt{2}} \\- \frac{1}{\sqrt{2}}\\0\end{bmatrix}, \ ||q_{3}||_{2}=1$$，满足条件，且由于 $$q_{3}$$和 $$k_{1}$$线性相关，因此注意力也是最大的。
考虑到 
$$
\sqrt{d}=2, s_{i}=\frac{q_{3} \cdot k_{i}}{2}, s_{1}=\frac{\sqrt{2}}{2}, s_{2}=s_{3}=-\frac{\sqrt{2}}{4}
$$
那么 
$$
\alpha_{1}=\frac{\exp(s_{1})}{\sum_{i=1}^{3}\exp{s_{i}}} \approx 0.591
$$
$$
\alpha_{2},\alpha_{3}=\frac{\exp(s_{2})}{\sum_{i=1}^{3}\exp{s_{i}}} \approx 0.205
$$
最终 
$$
y_{3}=0.591*v_{1}+0.205*v_{2}+0.205*v_{3}
$$

> 设生成了 token“He”之后，计算到这一层时，$$k_{4} = (−1, 0, −1, −1)^{T} $$。当 GPT 模型预测翻译的第二个 token（英文单词）“likes”的时候，它同时需要“喜欢”和“He”的信息（因为“likes”是第三人称单数形式），此时多头自注意力机制可以胜任。假设以 query，key，value向量的前两维和后两维作为两个自注意力头（Heads）的特征向量，请写出向量 $$q_{4}$$ 的一个取值，满足 $$q_{4}$$ 的 2 范数不超过 1，且在第一个自注意力头中与第二个 token（“喜欢”）的自注意力权重（相比别的 token）最大，在第二个自注意力头中与第四个 token（“He”）的自注意力权重最大，并写出此时 $$y_{4}$$ 的表达式。

和2同理，不妨先令 $$q_{4}=\frac{k_{4}}{||k_{4}||_{2}}=\begin{bmatrix}-\frac{1}{\sqrt{3}} \\0\\ -\frac{1}{\sqrt{3}} \\ - \frac{1}{\sqrt{3}}\end{bmatrix}$$
设 $$header1$$是前两维，$$header2$$是后两维；
$$
\alpha = (0,-0.577,0,0.577)
\beta = (0.577,-1.155,0.577,1.155)
$$
不难发现$$header1$$的自注意力权重在第4个token而不是第2个token，观察到第2个token的 $$k_{2:(1:2)}=(1,0)$$，因此把 $$q_{4}$$改为 
$$
q_{4}=\begin{bmatrix}\frac{1}{\sqrt{2}} \\0\\ -\frac{1}{2} \\- \frac{1}{2}\end{bmatrix}
$$
此时满足条件而且：
$$
y_{4}=[\sum_{j=1}^{4}(\alpha_{j}v_{j:(1:2)}),\sum_{j=1}^{4}(\beta_{j}v_{j:(3:4)})]
$$

`P3`
$$proof:$$
设初始化的w为$$w_{0}$$,共经过t轮更新后收敛，更新得到的w依次为 $$w_{1},\dots,w_{t}$$. 并且使用的数据分别为 $$(x1,y1),\dots,(xt,yt)$$。
首先由感知机更新条件
$$
w_{t} = w_{t-1} + y_{t}x_{t} \to w_{t}^{T}w^{*} = w_{t-1}^{T}w^{*} + y_{t}x_{t}^{T}w^{*} \ (i)
$$
和margin条件的线性可分假设
$$
y_{t}x_{t}^{T}w^{*} \ge \gamma \ (ii)
$$
把(i)(ii)做t次迭代累加，可以得到(1)式：
$$
w_{t}^{T}w^{*}=w_{t-1}^{T}w^{*}+y_{t}w^{*T}x_{t} \ge w_{t-1}^{T}w^{*}+\gamma \ge w_{0}^{T}w^{*}+\gamma \ (1)
$$
利用归一化范数的性质，可得：
$$
y_{t}^{2}(\left \| x_{t} \right \|)^{2} \le 1*1 = 1 \ (iii)
$$
配合更新条件：
$$
y_{t}w_{t-1}^{T}x_{t} \le 0 \ (iv)
$$
还是迭代t次，可以得到第二个不等式：
$$
(\left \| w_{t} \right \|)^{2} = (w_{t-1}+y_{t}x_{t}) \cdot (w_{t-1}+y_{t}x_{t}) = (\left \| w_{t-1} \right \|)^{2} + y_{t}^{2}(\left \| x_{t} \right \|)^{2} + 2y_{t}w_{t-1}^{T}x_{t} \le (\left \| w_{t-1} \right \|)^{2} + 1 \le (\left \| w_{0} \right \|)^{2} + t \  (2)
$$
由Cauchy-Schwarz 不等式，可知：
$$
t \le \frac{(\left \| w^{*} \right \|)^{2}(\left \| w_{0} \right \|)^{2}}{\gamma^{2}}
$$
在 $$ \left \| w^{*} \right \| = 1, \left \| w_{0} \right \| = 0$$时得证。

##### 位置敏感 FFN Position-wise FFN

![image-20250613230616325](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613230616325.png)

##### 残差连接 Residual Connection

![image-20250613230700695](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613230700695.png)

##### 层归一化 Layer Normalization

![image-20250613230753943](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613230753943.png)

##### 位置编码 Positional Encoding

![image-20250613230854251](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613230854251.png)

![image-20250613230909762](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613230909762.png)

##### Transformer的架构

![image-20250613231046352](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250613231046352.png)

## 4. 推理 Probabilistic Reasoning

+ **概率推理**（Probabilistic Reasoning）是**建模**（Modeling）和**推断**（Inference）的组合过程
  + 建模：找到概率在特征上的联合分布$$p(x_1,x_2,\dots,x_D)$$
    + 贝叶斯网络（Bayesian networks）
  + 推断：计算概率在特征上的条件分布$$p(R|C)$$
    + 变量消除法（Elimination methods）
    + 隐变量模型（Latent variable models）
    + 变分方法（Variational methods）
    + 采样方法（Sampling methods）

### 4.1 图模型 Graphical Networks

#### 4.1.1 贝叶斯网络 Bayesian Networks

+ 按照概率的乘法规则，任何联合概率分布可以写成k个条件分布的乘积

  + 本质上是对依赖的因式分解

+ **贝叶斯网络**是一个**有向无环图**（DAG），它将联合分布指定为局部条件分布的乘积，每个节点对应一个条件分布：、
  $$
  p(x_1,\dots,x_k)=\prod_{s=1}^{K}p(x_s|x_{\Gamma(s)})
  $$

  + 其中$$\Gamma(s)$$是节点父节点集，即每个节点的条件概率只依赖其父亲

+ 贝叶斯网络的化简：对地震（E）盗窃（B）警报（A）报道（R）模型

  1. 选择顺序：$$p(A,R,E,B)=p(A|R,E,B)p(R|E,B)p(E|B)p(B)$$
  2. 使用独立假设简化分布：$$p(A,R,E,B)=P(A|E,B)p(R|E)p(E)p(B)$$
  3. 将展开式按因子转化为概率图模型

  ![image-20250614102908841](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614102908841.png)

  4. 将条件概率转化为最大联合概率的边缘概率

  $$
  p(B=1|A=1,R=1)=\frac{\sum_{E,R}p(B=1,E,A=1,R)}{\sum_{B,E,R}(B,E,A=1,R)}=0.5
  $$

+ **多项式回归**（Polynomial Regression）

![image-20250614103415591](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614103415591.png)

### 4.2 条件独立 Conditional Independence

#### 4.2.1 D-分离条件 D-separation

+ **独立随机变量**（independent random variables）

  + $$
    p_{XY}(x,y)=p_{X}(x)p_{Y}(y) \forall x \in X,y\in Y
    $$

  + 记作$$X \perp Y$$

+ **条件独立**（conditionally independent）：在确定条件变量的值是，概率独立分布

  + $$
    p_{XY|Z}(x,y|z)=p_{X|Z}(x|z)p_{Y|Z}(y|z)
    $$
  + 或者表示为x的分布仅和z有关，和y无关
  + $$
    {\color{red}p_{X|YZ}(x|y,z)=p_{X|Z}(x|z)}
    $$

  + 记作$$X\perp Y | Z$$
  
+ **头对头模型**（head-to-head model）

![image-20250614104257212](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614104257212.png)
  + 在(d)中，$$A \perp B$$

    + $$
      p(A,B)=\sum_{C}p(A,B,C)=\sum_{C}P(A)P(B)P(C|A,B)\\
      =p(A)p(B)\sum_{C}P(C|A,B)=p(A)p(B)
      $$

  + 在(a)(b)(c)中，$$A\perp B | C$$

    + (a):
      $$
      p(A,B|C)=\frac{p(A|C)p(B|C)p(C)}{p(C)}=p(A|C)p(B|C)
      $$

    + (b):Bayers
      $$
      p(A,B|C)=\frac{p(A|C)p(C|B)p(B)}{p(C)}=p(A|C)p(B|C)
      $$

    + (c)=(b)

  + 在(d)中，AB**边缘独立但对于C条件不独立**

    + 显然，给定C后A,B存在后验概率，必然就不独立了

  + **联通路径**

    + 考虑贝叶斯网络中A,B（点或点集）之间的一条无向路径，在给定条件集C下，路径是连通的：如果它属于这些情况之一：
      + 所有非头对头节点都不属于C
      + 任意头对头节点（或者头对头节点的后代）属于C


![image-20250614110004258](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614110004258.png)

+ **D-分离原理**（D-separation）
  + 对于**点集**A,B，在给定**条件点集**C的情况下，A,B**不连接**等价于
    1. A 和 B 是被 C **D分离的**
    2. $${\color{red}(A\perp B)|C}$$
  + ![image-20250614110455170](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614110455170.png)
  + 左侧头对头节点c的（已观测）后代d属于C，连接，D不分离
  + 右侧有非头对头节点e属于C，不连接，D-条件分离
+ **马尔科夫毯**（Markov Blanket）：确定x的分布需要的所有条件构成的集合
+ **马尔科夫边界**（Markov Boundary）：最小的马尔科夫毯，包括
  + **父节点们**：阻断所有直接“向上”或“向下”链式路径进入 X
  + **子节点们**：阻断 XXX 经过自己再向下传播的不确定性
  + **子节点的其它父节点（共父）**：阻断所有可能被 **V-结构激活**（头对头）的路径
  + 在把马尔科夫边界设置为条件后，点集中的其他任何节点都无法与被毯包裹的节点连接
+ 联合分布的因子模型

$$
p(x_1,x_2,\dots,x_n)=\prod_{s\in F}\phi_s(x_s)
$$

 + 在贝叶斯网络中，因子可以表示为

   + $$
     F=\{s\sup\Gamma(s)\forall s\}\\
     \phi_s=p(x_s|x_{\Gamma(s)})
     \\ p(\vec{x})=\prod_{s=1}^{N}p(x_s|x_{\Gamma(s)})
     $$

     


### 4.3 图模型推理 Inference in Graphical Models

#### 4.3.1 变量消除 Variable Elimination

+ **边际推理**（Marginal inference）：求所有条件加和后的概率

  + $$
    p(y=1)=\sum_{x_1}\dots\sum_{x_n}p(y=1,x_1,\dots,x_n)
    $$

  + **最大后验推理**（Maximum a posteriori (MAP) inference）：求一组条件使得这组条件下的概率最大

    + $$
      \underset{x_1,\dots,x_n}{\max}p(y=1,x_1,\dots,x_n)
      $$
  
+ **离散状态下的边际推理**

1.  对于一条简单的马尔科夫链$$x_1\to x_2\to\dots x_n$$（变量的取值有k个）

2. 展开联合分布和因子

3. $$
   p(x_n)=\sum_{x_1}\dots\sum_{x_{n-1}}p(x_1,\dots,x_n)=\sum_{x_1}\dots\sum_{x_{n-1}}p(x_1)\prod_{i=2}^{n}p(x_i|x_{i-1})\\
   =\sum_{x_{n-1}}p(x_n|x_{n-1})\sum_{x_{n-2}}p(x_{n-1}|x_{n-2})\dots\sum_{x_1}p(x_2|x_1)p(x_1)
   $$

4. 递推地计算因子
   $$
   \tau(x_1=s)=p(x_1=s)
   \\ \tau(x_t=s)=\sum_{x_{t-1}}p(x_t=s|x_{t-1})\tau(x_{t-1})
   \\ \tau(x_n)=p(x_n)
   $$

5. 复杂度是吓人的$$O(k_{n-1})+O(nk^2)$$

##### 因子操作 Operations

* 乘积（Product）：乘积变量范围内的取值

  +  例子：$$\phi_{3}(a,b,c)=\phi_1(a,b)\times \phi_2(b,c)$$

  * $$
    \phi_3(x_c)=\phi_1(x_c^{(1)})\times\phi_{2}(x_{c}^{(2)})
    $$

* 边缘化（Marginalization）：将关于两个变量的因子边缘化为一个新的因子

  * $$
    \tau(x)=\sum_{y}\phi(x,y)
    $$

##### 排序 Ordering

+ 对变量进行不同的排序可能会显著改变变量消除算法的运行时间。
+ 找到最佳排序是NP难问题

##### 变量消除算法 Variable Elimination

![image-20250614125232815](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614125232815.png)

```python
1.  初始因子集合  F ← CPTs , 并把证据信息塞入相应因子(删列归一化)
2.  for Zi in Z:                # 逐个消除
       FZi ← {f∈F | Zi∈vars(f)}          # 找到包含 Zi 的全部因子
       g  ← ∏f∈FZi f                     # 相乘
       h  ← Σ_{Zi} g                     # 对 Zi 求和 (marginalize)
       F  ← (F \ FZi) ∪ {h}              # 旧因子删掉，新因子加入
3.  输出结果  R = 归一化( ∏f∈F f )       # 剩下因子全乘即 P(Q|E)

```

1. 从拓扑排序中挑选出变量$$x_{i}$$
2. 对含有$$x_i$$的所有因子进行乘积运算（消去形如$$p(x_2|x_1,x_3)p(x_1)$$的隐变量$$x_1$$）
3. 对含有$$x_i$$的所有因子进行边缘化运算（消去$$\sum$$）
4. 使用新的因子代替旧因子

![image-20250614130056664](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614130056664.png)

简单而言，就是将条件概率和求和全部消去，只留下“箭头指向”的“显变量”

##### 引入证据 Introducing Evidence

![image-20250614130254420](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614130254420.png)

### 4.4 概率 Probability Basics

+ **贝叶斯方法**（Bayesian Approach）是我们对参数的认识，在贝叶斯方法中将参数视为随机变量
  + 需要使用先验知识决定$$\theta$$的先验分布，根据数据计算后验分布

![image-20250614131603705](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614131603705.png)

+ **贝叶斯公式**（Bayes Rule）：数据集是D，模型参数是$$\theta$$

+ $$
  p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}
  $$

  + $$
    p(\theta|D)\propto p(D|\theta)p(\theta)
    $$

+ 

+ 伯努利分布：$$p(x=1|q)=q,p(x=0|q)=1-q$$
+ 高斯分布

$$
p(x|\mu,\Sigma)=\frac{1}{\sqrt{|2\pi\Sigma|}}\exp{(\frac{-1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}
$$

+ 参数模型的似然函数：模型对于采样的表达能力（生成这n条样本的概率）

  + n条样本概率的连乘取对数（数值稳定）

  + $$
    \log p(\mathcal{D};\hat{\theta})=\sum_{n=1}^{N}\log p (z_n;\hat{\theta})
    $$

  + 
  
+ **最大似然估计**（Maximum Likelihood Estimation）：一组参数使得样本的采样概率达到最大

  + ![image-20250614132659168](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614132659168.png)


##### 高斯分布的最大似然估计 MLE for Gaussian Distribution

1. 已知高斯分布采样的概率，密度求出高斯分布采样的对数密度（防溢出）和对数联合密度（多条样本）

![image-20250614133134928](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614133134928.png)

2. 确定优化采样的目标函数：对数联合密度函数，使用梯度下降优化

![image-20250614133318195](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614133318195.png)

3. 得到封闭形式的参数解

![image-20250614133352660](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614133352660.png)

+ **贝叶斯决策原理**（Bayes Decision Rule）：

+ $$
  h(x)=\underset{y\in \mathcal{Y}}{\arg\max}[p(Y=y|X=x)]
  $$

### 4.5 判别模型 Discriminative Models

![image-20250614133917841](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614133917841.png)

![image-20250614134158431](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614134158431.png)

![image-20250614134309979](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614134309979.png)

+ 小样本学习中需要很强的先验，预训练大语言模型是一个很强的先验

![image-20250614134623746](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614134623746.png)

### 4.6 生成模型 Generative Models

#### 4.6.1 朴素贝叶斯分类器 Naïve Bayes Classifier

+ 寻找高维数据的分布，尤其是使用y推理x的后验是困难的，需要建模维度间的依赖关系

+ **朴素贝叶斯分类器**假定变量之间的条件独立性

  + $$
    p(X=\vec{x}|Y=y)=\prod_{j=1}^{d}p(x_{j}|y)
    $$

+ 

+ 这样$$p(x_i|y)$$和$$p(y)$$就能在数据库中分开计算了
+ 当特征都离散时，可以使用朴素贝叶斯分类器
  + 当特征取(0,1)值时，可以假设$$p(x_j|y)$$和$$p(y)$$满足伯努利分布

![image-20250614140157243](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614140157243.png)

![image-20250614140209570](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614140209570.png)

+ 上面的计算只用了贝叶斯公式
+ 为了避免$$p(x_j=r_j|Y=+1)=0$$导致$$p(\vec{x}|Y=+1)$$生成不出来，可以在分数上形式添加一个$$\frac{1}{n}$$小量
+ ![image-20250614140749828](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614140749828.png)

#### 4.6.2 高斯判别分析 Gaussian Discriminant Analysis

+ 高斯判别模型的思路
  + 要求$$p(y|x)\propto p(x|y)p(y)$$，显然后面两个因子都求不出来
    + 假设$$p(y)$$是(0,1)上的伯努利分布
    + 假设$$p(x|y=0)$$和$$p(x|y=1)$$是**均值不同，协方差相同**的多元高斯分析

![image-20250614142337094](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614142337094.png)

|              | **生成模型 (Generative)**                                    | **判别模型 (Discriminative)**                                |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **建模目标** | **完整概率分布**：先验 $$p(y) +$$ 类条件 $$p(x\mid y)$$（或直接 $$p(x,y)$$） | **决策边界 / 条件概率**：直接刻画 $$p(y\mid x)$$ 或 $$\arg\max_y p(y\mid x)$$ |
| **推断方法** | 利用贝叶斯公式 $$p(y\mid x)=\dfrac{p(x\mid y)p(y)}{\sum_{y'}p(x\mid y')p(y')}$$ | 直接输出 $$p(y\mid x)$$ 或得分 $$f(x)$$ 再 softmax/sigmoid   |
| **典型模型** | Naive Bayes、LDA/QDA、GMM、HMM、GAN 堆叠的生成器             | Logistic/Softmax 回归、SVM、条件随机场 (CRF)、神经分类器、Boosting |

### 4.7 混合模型 Mixture Models and EM

#### 4.7.1 高斯混合模型 Gaussian Mixture Model

+ **高斯混合模型**试图使用高斯分布拟合所有的概率密度函数，需要学习的参数有
  + **簇系数**（Cluster probabilities）：$$\pi_i$$：对第i个高斯分布采样的频率（权重）
  + **簇均值**（Cluster means）：$$\mu_i$$：第i个高斯分布的均值
  + **簇协方差**（Cluster covariance matrices）：$$\sum_i$$：第i个高斯分布的协方差
+ 这样，混合分布实际上就是对高斯分布的加权和

![image-20250614143722461](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614143722461.png)

+ 高斯混合模型的似然函数是：

![image-20250614144239833](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614144239833.png)

![image-20250614144316928](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614144316928.png)

+ 注意到似然函数内部的z求和是不可处理的，所以要使用近似方法

#### 4.7.2 期望最大化 Expectation Maximization

![image-20250614144830535](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614144830535.png)

+ 学习和推断的定义
  + **学习问题**（ Learning problem）：给定数据集D，学习参数$$\theta$$
  + **推理问题**（ Inference problem）：给定样本x，输出隐变量分布z
  + 两个问题都需要最大化期望

![image-20250614145445474](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614145445474.png)

+ 编辑对数似然函数难以优化
+ 通常，完整的数据对数似然很容易优化
+ 然后最大化期望完整数据的对数似然

##### 证据下界 Evidence Lower Bound (ELBO)

![image-20250614150450597](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614150450597.png)

![image-20250614150512843](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614150512843.png)

+ EM算法最大化ELBO中的参数$$\theta$$和变分分布$$q$$

![image-20250614150715756](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614150715756.png)

![image-20250614151121475](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614151121475.png)

![image-20250614151452751](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614151452751.png)

##### EMM for MAP 看不懂

### 4.8 概率主题模型 Probabilistic Topic Model

#### 4.8.1 狄利克雷多项式模型 Dirichlet-Multinomial Model

##### Beta-Bernoulli 模型

+ 贝塔分布：**伯努利分布**的共轭先验

+ $$
  f(\phi|\alpha,\beta)=\frac{1}{B(\alpha,\beta)}\phi^{\alpha-1}(1-\phi)^{\beta-1}
  $$

+ Beta-Bernoulli模型

  + 先使用Beta分布对参数进行先验分布，再使用伯努利分布进行实际测试

![image-20250614152814542](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614152814542.png)

| 元素           | 数学形式                                | 直观含义                            |
| -------------- | --------------------------------------- | ----------------------------------- |
| **潜在参数 θ** | θ∈(0,1)                                 | 事件“成功”(1) 的真实概率            |
| **先验分布**   | θ ∼ **Beta(α, β)**                      | 事前相信成功次数≈α–1，失败次数≈β–1  |
| **观测数据**   | x1,…,xNx_1, …, x_N, xi∈{0,1}x_i∈\{0,1\} | N 次独立试验结果                    |
| **似然函数**   | (p(x                                    | θ)=\prod_{i} θ^{x_i},(1-θ)^{1-x_i}) |

##### Dirichlet-Multinomial模型

+ **多项式分布**：（Multinomial Distribution）Mult$$(n,\theta)$$

$$
p(n_1,n_2,\dots,n_k)=\frac{n!}{n_1!n_2!\dots n_k!}\theta_{1}^{n_1}\dots\theta_{k}^{n_k}
$$

+ **狄利克雷分布**：Beta分布的K类别版本

$$
p(\vec{\theta}|\alpha)=\frac{1}{B(\alpha)}\prod_{k=1}^{K}\theta_{k}^{\alpha_{k}-1},B(\alpha)=\frac{\prod_{k=1}^{K}\Gamma(\alpha_{k})}{\Gamma(\sum_{k=1}^{K}\alpha_k)}
$$

![image-20250614153948530](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614153948530.png)

![image-20250614154137235](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614154137235.png)

#### 4.8.2 隐狄利克雷分配 Latent Dirichlet Allocation*

![image-20250614160109595](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614160109595.png)

- 从狄利克雷分布 α 中生成文档i的主题分布 θi ；
- 从主题的多项式分布 θi 中取样生成文档i第j个词的主题 Zi,j ；
- 从狄利克雷分布 η 中取样生成主题 Zi,j 对应的词语分布 βi,j ；
- 从词语的多项式分布 βi,j 中采样最终生成词语 Wi,j .

### 4.9 变分推理 Variational Inference

#### 4.9.1 均场变分推理 Mean Field Variational Inference（略

#### 4.9.2 LDA变分推理 Variational Inference for LDA（略

#### 4.10 变分自编码器 Variational Autoencoder

+ **自编码器**（Autoencoders）
  + 从未标记的训练数据中学习低维特征表示的无监督方法

![image-20250614160527203](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614160527203.png)

+ **变分自编码器**（Variational Autoencoders (VAE)）

  + 变分自编码器的特征是一个隐变量

  + 难点：对于生成器$$p_{\theta}(x|z)$$

    + 要计算生成器的对数似然$$\log p_{\theta}(x)=\log \int p_{\theta}(x|z)p(z)dz$$
    + 真实后验$$p_{\theta}(z|x)$$算不出来
    + 使用推断器的$$q_{\phi}(z|x)$$代替

  + 这样，就可以使用EBEO逼近真实后验

  + ![image-20250614161503936](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614161503936.png)

  + VAE的训练关键是把 ELBO 变成“可写成网络+SGD 的损失”

  + ![image-20250614161935287](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614161935287.png)

  + ![image-20250614162001775](C:\Users\25848\AppData\Roaming\Typora\typora-user-images\image-20250614162001775.png)

  + 总体是：

  + ```sql
      ┌────── φ : Encoder (q) ────────┐
    x ──► μ(x),σ(x)  ──(+ ε·σ)──►  z ─────────► pθ(x|z) ──►  x̂
                               ▲                  └───── θ : Decoder (p) ───┘
                                ╰─────────────── KL vs p(z) ~ N(0,I)
    
    ```

    

